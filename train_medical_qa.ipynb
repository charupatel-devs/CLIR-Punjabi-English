{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-15T06:30:14.739058Z",
     "start_time": "2025-03-15T06:30:13.813524Z"
    }
   },
   "source": "pip install pandas torch transformers tqdm scikit-learn",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.6.0)\r\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.49.0)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (4.67.1)\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.1)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.12.0)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (76.0.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.29.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T10:43:48.562946Z",
     "start_time": "2025-03-18T10:43:44.682057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from IndicTransToolkit.IndicTransToolkit import IndicProcessor\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "d6183360afac41fa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charupatelbaghi/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T10:43:48.579617Z",
     "start_time": "2025-03-18T10:43:48.575760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    if qconfig == None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    translations = []\n",
    "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
    "        batch = input_sentences[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Preprocess the batch and extract entity mappings\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "        # Tokenize the batch and generate input encodings\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Generate translations using the model\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            generated_tokens = tokenizer.batch_decode(\n",
    "                generated_tokens.detach().cpu().tolist(),\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "\n",
    "        # Postprocess the translations, including entity replacement\n",
    "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ],
   "id": "a8dd4a4083715135",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T10:45:28.405867Z",
     "start_time": "2025-03-18T10:44:01.143105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load SciSpacy Medical Model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Load AI4Bharat Translation Models\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, None)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, None)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "# Translation Function\n",
    "def translate_text(text, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    try:\n",
    "        translations = batch_translate([text], src_lang, tgt_lang, model, tokenizer, ip)\n",
    "        return translations[0] if translations else None\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text, language=\"en\"):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "    return tokens\n",
    "\n",
    "# Improved Query Expansion (Medical Terms)\n",
    "def expand_query(tokens):\n",
    "    expanded_tokens = set(tokens)\n",
    "    for token in tokens:\n",
    "        if token in [\"medications\", \"medicine\", \"medicament\"]:\n",
    "            expanded_tokens.update([\"drug\", \"treatment\", \"prescription\", \"medication\"])\n",
    "        elif token == \"diabetes\":\n",
    "            expanded_tokens.update([\"diabetic\", \"insulin\", \"blood sugar\"])\n",
    "        elif token == \"hypertension\":\n",
    "            expanded_tokens.update([\"high blood pressure\", \"blood pressure\"])\n",
    "    return list(expanded_tokens)\n",
    "\n",
    "# Retrieve Top 2 Documents using TF-IDF and Cosine Similarity\n",
    "def retrieve_documents(query_tokens, documents):\n",
    "    \"\"\"\n",
    "    Uses TF-IDF Vectorization and Cosine Similarity to find the most relevant documents.\n",
    "    Always returns exactly 2 top-ranked documents.\n",
    "    \"\"\"\n",
    "    all_docs = documents + [\" \".join(query_tokens)]  # Append query as a \"document\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_docs)\n",
    "\n",
    "    # Compute similarity between the last item (query) and all documents\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "\n",
    "    # Rank documents by similarity\n",
    "    ranked_docs = sorted(\n",
    "        [(documents[i], similarity_scores[i]) for i in range(len(documents))],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nRetrieved Top 2 Documents with Scores:\")\n",
    "    for i, (doc, score) in enumerate(ranked_docs[:2]):\n",
    "        print(f\"{i+1}. Score: {score:.4f} | Content: {doc}\")\n",
    "\n",
    "    return [doc for doc, score in ranked_docs[:2] if score > 0.1]  # Return only top 2 relevant docs\n",
    "\n",
    "# Summarize Documents\n",
    "def summarize_document(doc_content):\n",
    "    sentences = doc_content.split(\". \")\n",
    "    return sentences[0] + \".\"\n",
    "\n",
    "# Main System Function\n",
    "def punjabi_english_clir(query, documents):\n",
    "    # Load translation models\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    # Step 1: Translate Punjabi to English\n",
    "    translated_query = translate_text(query, \"pan_Guru\", \"eng_Latn\", indic_en_model, indic_en_tokenizer, ip)\n",
    "    if not translated_query:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (Translation failed)\"]\n",
    "    print(f\"\\nTranslated Query (English): {translated_query}\")\n",
    "\n",
    "    # Step 2: Preprocess translated query\n",
    "    query_tokens = preprocess_text(translated_query, language=\"en\")\n",
    "    print(f\"\\nPreprocessed Tokens: {query_tokens}\")\n",
    "\n",
    "    # Step 3: Expand query\n",
    "    expanded_tokens = expand_query(query_tokens)\n",
    "    print(f\"\\nExpanded Tokens: {expanded_tokens}\")\n",
    "\n",
    "    # Step 4: Retrieve top 2 documents using TF-IDF and Cosine Similarity\n",
    "    retrieved_docs = retrieve_documents(expanded_tokens, documents)\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        print(\"\\n❌ No documents were retrieved. Check indexing or query matching.\")\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ।\"]\n",
    "\n",
    "    # Step 5: Summarize and translate back to Punjabi\n",
    "    punjabi_summaries = []\n",
    "    for i, content in enumerate(retrieved_docs, 1):\n",
    "        summary_en = summarize_document(content)\n",
    "\n",
    "        # Step 6: Translate back to Punjabi\n",
    "        summary_pa = translate_text(summary_en, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "        if summary_pa:\n",
    "            punjabi_summaries.append(f\"{i}. {summary_pa}\")\n",
    "        else:\n",
    "            punjabi_summaries.append(f\"{i}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "\n",
    "    return punjabi_summaries\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    documents = [\n",
    "        \"Diabetes medications include insulin and oral drugs like Metformin.\",\n",
    "        \"People with diabetes should monitor their blood sugar regularly.\",\n",
    "        \"A healthy diet and physical activity help manage diabetes effectively.\",\n",
    "        \"Doctors recommend prescription drugs for managing diabetes symptoms.\",\n",
    "        \"Insulin therapy is essential for some diabetic patients.\",\n",
    "        \"The best medications for diabetes treatment include Metformin and Insulin.\",\n",
    "        \"Patients with high blood sugar levels should consult a doctor for appropriate medications.\",\n",
    "        \"Hypertension medications include beta-blockers and ACE inhibitors.\",\n",
    "        \"Metformin is a widely used prescription drug for diabetes management.\"\n",
    "    ]\n",
    "\n",
    "    punjabi_query = \"ਕੀ ਮੈਂ ਸ਼ੂਗਰ ਲਈ ਕਿਹੜੀ ਦਵਾਈ ਲੈ ਸਕਦਾ ਹਾਂ?\"\n",
    "    results = punjabi_english_clir(punjabi_query, documents)\n",
    "\n",
    "    print(\"\\nਸੰਖੇਪ ਜਵਾਬ (Summaries in Punjabi):\")\n",
    "    for summary in results:\n",
    "        print(summary)\n"
   ],
   "id": "dd637a6251e4edb7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/charupatelbaghi/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
      "/Users/charupatelbaghi/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translated Query (English): What medications can I take for diabetes?\n",
      "\n",
      "Preprocessed Tokens: ['medications', 'take', 'diabetes']\n",
      "\n",
      "Expanded Tokens: ['take', 'medications', 'prescription', 'medication', 'diabetic', 'blood sugar', 'diabetes', 'insulin', 'treatment', 'drug']\n",
      "\n",
      "Retrieved Top 2 Documents with Scores:\n",
      "1. Score: 0.2813 | Content: The best medications for diabetes treatment include Metformin and Insulin.\n",
      "2. Score: 0.2332 | Content: Metformin is a widely used prescription drug for diabetes management.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 145\u001B[39m\n\u001B[32m    132\u001B[39m documents = [\n\u001B[32m    133\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mDiabetes medications include insulin and oral drugs like Metformin.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    134\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mPeople with diabetes should monitor their blood sugar regularly.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    141\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mMetformin is a widely used prescription drug for diabetes management.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    142\u001B[39m ]\n\u001B[32m    144\u001B[39m punjabi_query = \u001B[33m\"\u001B[39m\u001B[33mਕੀ ਮੈਂ ਸ਼ੂਗਰ ਲਈ ਕਿਹੜੀ ਦਵਾਈ ਲੈ ਸਕਦਾ ਹਾਂ?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m results = \u001B[43mpunjabi_english_clir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpunjabi_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    147\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mਸੰਖੇਪ ਜਵਾਬ (Summaries in Punjabi):\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    148\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m summary \u001B[38;5;129;01min\u001B[39;00m results:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 122\u001B[39m, in \u001B[36mpunjabi_english_clir\u001B[39m\u001B[34m(query, documents)\u001B[39m\n\u001B[32m    119\u001B[39m summary_en = summarize_document(content)\n\u001B[32m    121\u001B[39m \u001B[38;5;66;03m# Step 6: Translate back to Punjabi\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m summary_pa = \u001B[43mtranslate_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43msummary_en\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meng_Latn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpan_Guru\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43men_indic_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43men_indic_tokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m summary_pa:\n\u001B[32m    124\u001B[39m     punjabi_summaries.append(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msummary_pa\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 35\u001B[39m, in \u001B[36mtranslate_text\u001B[39m\u001B[34m(text, src_lang, tgt_lang, model, tokenizer, ip)\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtranslate_text\u001B[39m(text, src_lang, tgt_lang, model, tokenizer, ip):\n\u001B[32m     34\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m         translations = \u001B[43mbatch_translate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     36\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m translations[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m translations \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     37\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 54\u001B[39m, in \u001B[36mbatch_translate\u001B[39m\u001B[34m(input_sentences, src_lang, tgt_lang, model, tokenizer, ip)\u001B[39m\n\u001B[32m     52\u001B[39m \u001B[38;5;66;03m# Generate translations using the model\u001B[39;00m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m     generated_tokens = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     56\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     57\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmin_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m256\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     59\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     63\u001B[39m \u001B[38;5;66;03m# Decode the generated tokens into text\u001B[39;00m\n\u001B[32m     65\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m tokenizer.as_target_tokenizer():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2254\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[39m\n\u001B[32m   2246\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2247\u001B[39m         input_ids=input_ids,\n\u001B[32m   2248\u001B[39m         expand_size=generation_config.num_beams,\n\u001B[32m   2249\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2250\u001B[39m         **model_kwargs,\n\u001B[32m   2251\u001B[39m     )\n\u001B[32m   2253\u001B[39m     \u001B[38;5;66;03m# 13. run beam sample\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2254\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2255\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeam_scorer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2257\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2259\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2260\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2261\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2262\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2264\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001B[32m   2265\u001B[39m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[32m   2266\u001B[39m     beam_scorer = BeamSearchScorer(\n\u001B[32m   2267\u001B[39m         batch_size=batch_size,\n\u001B[32m   2268\u001B[39m         num_beams=generation_config.num_beams,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2274\u001B[39m         max_length=generation_config.max_length,\n\u001B[32m   2275\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3463\u001B[39m, in \u001B[36mGenerationMixin._beam_search\u001B[39m\u001B[34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001B[39m\n\u001B[32m   3460\u001B[39m     outputs = stack_model_outputs(outputs_per_sub_batch, \u001B[38;5;28mself\u001B[39m.config.get_text_config())\n\u001B[32m   3462\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Unchanged original behavior\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3463\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   3465\u001B[39m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[32m   3466\u001B[39m model_kwargs = \u001B[38;5;28mself\u001B[39m._update_model_kwargs_for_generation(\n\u001B[32m   3467\u001B[39m     outputs,\n\u001B[32m   3468\u001B[39m     model_kwargs,\n\u001B[32m   3469\u001B[39m     is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   3470\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2-en-indic-1B/c0463b57547962728d34ab12734e9e9ceae6fb08/modeling_indictrans.py:1717\u001B[39m, in \u001B[36mIndicTransForConditionalGeneration.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1712\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m decoder_input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1713\u001B[39m         decoder_input_ids = shift_tokens_right(\n\u001B[32m   1714\u001B[39m             labels, \u001B[38;5;28mself\u001B[39m.config.pad_token_id, \u001B[38;5;28mself\u001B[39m.config.decoder_start_token_id\n\u001B[32m   1715\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1717\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1718\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1719\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1720\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1721\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_outputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1722\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1723\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1724\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1725\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1726\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1727\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1728\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1729\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1730\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1731\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1732\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1733\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1734\u001B[39m lm_logits = \u001B[38;5;28mself\u001B[39m.lm_head(outputs[\u001B[32m0\u001B[39m])\n\u001B[32m   1736\u001B[39m masked_lm_loss = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2-en-indic-1B/c0463b57547962728d34ab12734e9e9ceae6fb08/modeling_indictrans.py:1614\u001B[39m, in \u001B[36mIndicTransModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1607\u001B[39m     encoder_outputs = BaseModelOutput(\n\u001B[32m   1608\u001B[39m         last_hidden_state=encoder_outputs[\u001B[32m0\u001B[39m],\n\u001B[32m   1609\u001B[39m         hidden_states=encoder_outputs[\u001B[32m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(encoder_outputs) > \u001B[32m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1610\u001B[39m         attentions=encoder_outputs[\u001B[32m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(encoder_outputs) > \u001B[32m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1611\u001B[39m     )\n\u001B[32m   1613\u001B[39m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1614\u001B[39m decoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1615\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1616\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1617\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_outputs\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1618\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1619\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1620\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1621\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1622\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1623\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1624\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1625\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1626\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1629\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict:\n\u001B[32m   1630\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m decoder_outputs + encoder_outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2-en-indic-1B/c0463b57547962728d34ab12734e9e9ceae6fb08/modeling_indictrans.py:1484\u001B[39m, in \u001B[36mIndicTransDecoder.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1471\u001B[39m         layer_outputs = torch.utils.checkpoint.checkpoint(\n\u001B[32m   1472\u001B[39m             create_custom_forward(decoder_layer),\n\u001B[32m   1473\u001B[39m             hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1481\u001B[39m             \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1482\u001B[39m         )\n\u001B[32m   1483\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1484\u001B[39m         layer_outputs = \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1485\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1486\u001B[39m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1487\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1488\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1489\u001B[39m \u001B[43m            \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1490\u001B[39m \u001B[43m                \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m   1491\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1492\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcross_attn_layer_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1493\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m   1494\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m   1495\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m   1496\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1497\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1498\u001B[39m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1499\u001B[39m \u001B[43m            \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1500\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1502\u001B[39m     hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1504\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m skip_the_layer:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2-en-indic-1B/c0463b57547962728d34ab12734e9e9ceae6fb08/modeling_indictrans.py:925\u001B[39m, in \u001B[36mIndicTransDecoderLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001B[39m\n\u001B[32m    923\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.normalize_before:\n\u001B[32m    924\u001B[39m     hidden_states = \u001B[38;5;28mself\u001B[39m.final_layer_norm(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m925\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.activation_fn(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    926\u001B[39m hidden_states = F.dropout(\n\u001B[32m    927\u001B[39m     hidden_states, p=\u001B[38;5;28mself\u001B[39m.activation_dropout, training=\u001B[38;5;28mself\u001B[39m.training\n\u001B[32m    928\u001B[39m )\n\u001B[32m    929\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.fc2(hidden_states)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T10:56:38.487913Z",
     "start_time": "2025-03-18T10:51:54.826227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Required NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load SciSpacy Medical Model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Load AI4Bharat Translation Models\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, None)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, None)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "# Translation function\n",
    "def translate_text(text, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    try:\n",
    "        translations = batch_translate([text], src_lang, tgt_lang, model, tokenizer, ip)\n",
    "        return translations[0] if translations else None\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [t for t in tokens if t.isalnum() and t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# ✅ **Improved Query Expansion (More Precise Synonyms)**\n",
    "def expand_query_with_wordnet(tokens):\n",
    "    \"\"\"\n",
    "    Expands query only with medically relevant synonyms.\n",
    "    Avoids irrelevant words like \"dainty\", \"kickshaw\", \"yield\".\n",
    "    \"\"\"\n",
    "    expanded_tokens = set(tokens)\n",
    "\n",
    "    # **Define medically relevant synonyms**\n",
    "    medical_expansion = {\n",
    "        \"pills\": [\"tablets\", \"capsules\", \"medication\", \"drug\"],\n",
    "        \"diabetes\": [\"diabetic\", \"blood sugar\", \"insulin\"],\n",
    "        \"medications\": [\"drugs\", \"prescription\", \"treatment\"],\n",
    "        \"treat\": [\"therapy\", \"cure\", \"medicate\"]\n",
    "    }\n",
    "\n",
    "    # **Use predefined medical synonyms**\n",
    "    for token in tokens:\n",
    "        if token in medical_expansion:\n",
    "            expanded_tokens.update(medical_expansion[token])\n",
    "\n",
    "    # **Restrict WordNet Expansion to Medical Terms Only**\n",
    "    allowed_categories = {\"medication\", \"treatment\", \"drug\", \"medicine\", \"therapy\", \"disease\"}\n",
    "    for token in tokens:\n",
    "        for syn in wn.synsets(token, pos=wn.NOUN):  # Only use noun synonyms\n",
    "            for lemma in syn.lemmas():\n",
    "                synonym = lemma.name().replace(\"_\", \" \").lower()\n",
    "                if synonym.isalnum() and synonym in allowed_categories:  # ✅ Filter irrelevant words\n",
    "                    expanded_tokens.add(synonym)\n",
    "\n",
    "    return list(expanded_tokens)\n",
    "\n",
    "# ✅ **Retrieve Top 2 Documents (Always Returns Translations)**\n",
    "def retrieve_documents(query_tokens, documents):\n",
    "    \"\"\"\n",
    "    Uses TF-IDF Vectorization with improved similarity calculation.\n",
    "    Always returns exactly 2 top-ranked documents and translates them.\n",
    "    \"\"\"\n",
    "    all_docs = documents + [\" \".join(query_tokens)]\n",
    "    vectorizer = TfidfVectorizer(norm='l2')  # ✅ Normalizing TF-IDF scores\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_docs)\n",
    "\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "\n",
    "    ranked_docs = sorted(\n",
    "        [(documents[i], similarity_scores[i]) for i in range(len(documents))],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    top_docs = ranked_docs[:2]  # ✅ Always return 2 documents, even with low scores\n",
    "\n",
    "    print(\"\\nRetrieved Top 2 Documents with Scores:\")\n",
    "    for idx, (doc, score) in enumerate(top_docs):\n",
    "        print(f\"{idx+1}. Score: {score:.4f} | Content: {doc}\")\n",
    "\n",
    "    return [doc for doc, score in top_docs]  # ✅ No threshold check, always return top 2\n",
    "\n",
    "# Summarize documents\n",
    "def summarize_document(doc_content):\n",
    "    return doc_content.split(\". \")[0] + \".\"\n",
    "\n",
    "# Main Punjabi-English-Punjabi retrieval system\n",
    "def punjabi_english_clir(query, documents):\n",
    "    # Load translation models\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    # Step 1: Translate Punjabi to English\n",
    "    translated_query = translate_text(query, \"pan_Guru\", \"eng_Latn\", indic_en_model, indic_en_tokenizer, ip)\n",
    "    if not translated_query:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (Translation failed)\"]\n",
    "    print(f\"\\nTranslated Query (English): {translated_query}\")\n",
    "\n",
    "    # Step 2: Preprocess translated query\n",
    "    query_tokens = preprocess_text(translated_query)\n",
    "    print(f\"\\nPreprocessed Tokens: {query_tokens}\")\n",
    "\n",
    "    # Step 3: Expand query using refined WordNet synonyms\n",
    "    expanded_tokens = expand_query_with_wordnet(query_tokens)\n",
    "    print(f\"\\nExpanded Tokens with WordNet: {expanded_tokens}\")\n",
    "\n",
    "    # Step 4: Retrieve top 2 documents using normalized TF-IDF and Cosine Similarity\n",
    "    retrieved_docs = retrieve_documents(expanded_tokens, documents)\n",
    "\n",
    "    # Step 5: Summarize and translate back to Punjabi (Always Translate)\n",
    "    punjabi_summaries = []\n",
    "    for idx, content in enumerate(retrieved_docs, 1):\n",
    "        summary_en = summarize_document(content)\n",
    "\n",
    "        # Translate summary back to Punjabi\n",
    "        summary_pa = translate_text(summary_en, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "        if summary_pa:\n",
    "            punjabi_summaries.append(f\"{idx}. {summary_pa}\")\n",
    "        else:\n",
    "            punjabi_summaries.append(f\"{idx}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "\n",
    "    return punjabi_summaries\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    documents = [\n",
    "        \"Diabetes medications include insulin and oral drugs like Metformin.\",\n",
    "        \"People with diabetes should monitor their blood sugar regularly.\",\n",
    "        \"A healthy diet and physical activity help manage diabetes effectively.\",\n",
    "        \"Doctors recommend prescription drugs for managing diabetes symptoms.\",\n",
    "        \"Insulin therapy is essential for some diabetic patients.\",\n",
    "        \"Medications in the form of pills or tablets are often prescribed for diabetes.\",\n",
    "        \"Capsules and oral medications like Metformin help control diabetes.\"\n",
    "    ]\n",
    "\n",
    "    punjabi_query = \"ਕੀ ਮੈਂ ਗੋਲੀਆਂ ਲਈ ਸਕਦਾ ਹਾਂ ਸ਼ੂਗਰ ਦੇ ਇਲਾਜ ਲਈ?\"\n",
    "    results = punjabi_english_clir(punjabi_query, documents)\n",
    "\n",
    "    print(\"\\nਸੰਖੇਪ ਜਵਾਬ (Summaries in Punjabi):\")\n",
    "    for summary in results:\n",
    "        print(summary)\n"
   ],
   "id": "528edd2f019a7e9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translated Query (English): Can I take pills to treat diabetes?\n",
      "\n",
      "Preprocessed Tokens: ['take', 'pills', 'treat', 'diabetes']\n",
      "\n",
      "Expanded Tokens with WordNet: ['take', 'tablets', 'capsules', 'medication', 'treat', 'therapy', 'cure', 'diabetes', 'medicate', 'diabetic', 'blood sugar', 'insulin', 'pills', 'drug']\n",
      "\n",
      "Retrieved Top 2 Documents with Scores:\n",
      "1. Score: 0.2225 | Content: Insulin therapy is essential for some diabetic patients.\n",
      "2. Score: 0.1705 | Content: People with diabetes should monitor their blood sugar regularly.\n",
      "\n",
      "ਸੰਖੇਪ ਜਵਾਬ (Summaries in Punjabi):\n",
      "1. ਸ਼ੂਗਰ ਦੇ ਕੁੱਝ ਮਰੀਜ਼ਾਂ ਲਈ ਇਨਸੁਲਿਨ ਥੈਰੇਪੀ ਜ਼ਰੂਰੀ ਹੈ। \n",
      "2. ਸ਼ੂਗਰ ਵਾਲੇ ਲੋਕਾਂ ਨੂੰ ਆਪਣੇ ਬਲੱਡ ਸ਼ੂਗਰ ਦੀ ਨਿਯਮਤ ਤੌਰ'ਤੇ ਨਿਗਰਾਨੀ ਕਰਨੀ ਚਾਹੀਦੀ ਹੈ। \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "doing!! webscraping a try",
   "id": "b9fbeacd2d8f96f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:27:43.820633Z",
     "start_time": "2025-03-18T11:27:33.331926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import scispacy\n",
    "import spacy\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "# Required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load SciSpacy Medical Model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# ✅ Function to Perform Google Search\n",
    "def google_search(query, num_results=5):\n",
    "    \"\"\"\n",
    "    Uses Google Search (via API or Scraping) to retrieve top results.\n",
    "    \"\"\"\n",
    "    search_url = f\"https://www.google.com/search?q={query}&num={num_results}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    links = []\n",
    "    for result in soup.find_all('a', href=True):\n",
    "        href = result['href']\n",
    "        match = re.search(r'/url\\?q=(.*?)&', href)\n",
    "        if match:\n",
    "            links.append(match.group(1))\n",
    "\n",
    "    return links[:num_results]\n",
    "\n",
    "# ✅ Function to Scrape Web Page Content\n",
    "def scrape_webpage(url):\n",
    "    \"\"\"\n",
    "    Extracts text content from a given URL.\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extracting readable text\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \" \".join([para.get_text() for para in paragraphs if para.get_text()])\n",
    "\n",
    "    return text if len(text) > 300 else None  # Filter out short results\n",
    "\n",
    "# ✅ NLP-Based Document Ranking\n",
    "def rank_documents(query, documents):\n",
    "    \"\"\"\n",
    "    Uses TF-IDF Vectorization and Cosine Similarity to rank web search results.\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return []\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents + [query])\n",
    "\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "\n",
    "    ranked_docs = sorted(\n",
    "        [(documents[i], similarity_scores[i]) for i in range(len(documents))],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    return ranked_docs[:2]  # ✅ Return only top 2 documents\n",
    "\n",
    "# ✅ Text Summarization (Extractive)\n",
    "def summarize_text(text, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Extracts the most relevant sentences using NLP-based scoring.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    # Use TF-IDF to rank sentences\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "    ranked_sentences = sorted(\n",
    "        [(sentences[i], scores[i]) for i in range(len(sentences))],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    return \" \".join([s[0] for s in ranked_sentences[:num_sentences]])\n",
    "\n",
    "# ✅ Load Translation Models\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, None)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, None)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "# ✅ Translation Function\n",
    "def translate_text(text, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    try:\n",
    "        translations = batch_translate([text], src_lang, tgt_lang, model, tokenizer, ip)\n",
    "        return translations[0] if translations else None\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ✅ Main Function: Web Search → Scraping → NLP Processing → Translation\n",
    "def web_search_clir(query):\n",
    "    # Load translation models\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    # ✅ Step 1: Translate Punjabi Query to English\n",
    "    translated_query = translate_text(query, \"pan_Guru\", \"eng_Latn\", indic_en_model, indic_en_tokenizer, ip)\n",
    "    if not translated_query:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (Translation failed)\"]\n",
    "    print(f\"\\n🔍 Translated Query (English): {translated_query}\")\n",
    "\n",
    "    # ✅ Step 2: Google Search for Relevant Links\n",
    "    search_results = google_search(translated_query)\n",
    "    print(\"\\n🔗 Retrieved Web Links:\")\n",
    "    for link in search_results:\n",
    "        print(link)\n",
    "\n",
    "    if not search_results:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (No web results found)\"]\n",
    "\n",
    "    # ✅ Step 3: Scrape Web Pages and Extract Text\n",
    "    extracted_texts = []\n",
    "    for link in search_results:\n",
    "        text = scrape_webpage(link)\n",
    "        if text:\n",
    "            extracted_texts.append(text)\n",
    "\n",
    "    if not extracted_texts:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (No readable content found)\"]\n",
    "\n",
    "    # ✅ Step 4: Rank Documents Using NLP\n",
    "    ranked_docs = rank_documents(translated_query, extracted_texts)\n",
    "\n",
    "    print(\"\\n📄 Top Ranked Documents (Post-Ranking):\")\n",
    "    for i, (doc, score) in enumerate(ranked_docs, 1):\n",
    "        print(f\"{i}. Score: {score:.4f}\")\n",
    "\n",
    "    if not ranked_docs:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (No relevant documents found)\"]\n",
    "\n",
    "    # ✅ Step 5: Summarize the Top Document\n",
    "    best_doc = ranked_docs[0][0]\n",
    "    summary_en = summarize_text(best_doc)\n",
    "\n",
    "    # ✅ Step 6: Translate Summary Back to Punjabi\n",
    "    summary_pa = translate_text(summary_en, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "\n",
    "    return [summary_pa if summary_pa else \"ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\"]\n",
    "\n",
    "# ✅ Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    punjabi_query = \"ਕੀ ਮੈਂ ਸ਼ੂਗਰ ਲਈ ਕਿਹੜੀ ਦਵਾਈ ਲੈ ਸਕਦਾ ਹਾਂ?\"\n",
    "    results = web_search_clir(punjabi_query)\n",
    "\n",
    "    print(\"\\n📢 **Final Answer in Punjabi:**\")\n",
    "    for res in results:\n",
    "        print(res)\n"
   ],
   "id": "d1227b7150d8069",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Translated Query (English): What medications can I take for diabetes?\n",
      "\n",
      "🔗 Retrieved Web Links:\n",
      "\n",
      "📢 **Final Answer in Punjabi:**\n",
      "ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (No web results found)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T13:25:37.902173Z",
     "start_time": "2025-03-18T13:25:37.878703Z"
    }
   },
   "cell_type": "code",
   "source": "api key-AIzaSyB9SV91pHITThVl-RrRaPMgoHnMcJuZMwM",
   "id": "3923a1993777a73",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1693684359.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mapi key-AIzaSyB9SV91pHITThVl-RrRaPMgoHnMcJuZMwM\u001B[39m\n        ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "91ad781c8d390f83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "31b02e4c84fe81bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
