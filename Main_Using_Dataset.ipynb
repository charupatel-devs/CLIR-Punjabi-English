{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-14T06:21:28.403080Z",
     "start_time": "2025-04-14T06:20:49.925278Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ‚úÖ Step 1: Load & Process Dataset\n",
    "file_path = \"Extracted_Medical_Q_A.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Debugging: Check Available Columns\n",
    "print(\"üìä Available Columns:\", df.columns)\n",
    "\n",
    "# ‚úÖ Ensure \"Question\" & \"Answer\" Columns Exist\n",
    "if \"Question\" not in df.columns or \"Clean_Answer\" not in df.columns:\n",
    "    raise KeyError(\"‚ùå Missing 'Question' or 'Clean_Answer' column in dataset.\")\n",
    "\n",
    "# ‚úÖ Step 2: Remove Duplicates & NaNs\n",
    "df = df.dropna(subset=[\"Question\", \"Clean_Answer\"]).drop_duplicates(subset=[\"Question\"]).reset_index(drop=True)\n",
    "\n",
    "# ‚úÖ Debugging: Print Extracted Questions\n",
    "print(\"\\nüì¢ Extracted Questions Sample:\")\n",
    "print(df[\"Question\"].head(10))\n",
    "\n",
    "# ‚úÖ Step 3: Load Pretrained BERT Model\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Lightweight & optimized for Q&A\n",
    "\n",
    "# ‚úÖ Step 4: Compute BERT Sentence Embeddings for Questions\n",
    "question_embeddings = bert_model.encode(df[\"Question\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# ‚úÖ Debugging: Check Shape of Embeddings\n",
    "print(\"\\nüìä Question Embeddings Shape:\", question_embeddings.shape)\n",
    "\n",
    "if question_embeddings.shape[0] == 0:\n",
    "    raise ValueError(\"‚ùå No question embeddings found. Check dataset processing!\")\n",
    "\n",
    "# ‚úÖ Step 5: Improved Token Processing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses input text:\n",
    "    - Removes stopwords\n",
    "    - Applies lemmatization\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase & tokenize\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "# ‚úÖ Step 6: Train TF-IDF Model on Processed Questions\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Question\"].apply(preprocess_text))\n",
    "\n",
    "# ‚úÖ Step 7: Hybrid Retrieval (BERT + TF-IDF)\n",
    "def retrieve_best_answer(input_question, top_k=3):\n",
    "    \"\"\"\n",
    "    Uses a hybrid approach (TF-IDF + BERT) to retrieve the best-matched question and return the corresponding answer.\n",
    "    \"\"\"\n",
    "    # üîπ Process input question\n",
    "    processed_input = preprocess_text(input_question)\n",
    "\n",
    "    # üîπ TF-IDF Similarity\n",
    "    input_vector = vectorizer.transform([processed_input])\n",
    "    tfidf_scores = np.dot(input_vector, tfidf_matrix.T).toarray().flatten()\n",
    "\n",
    "    # üîπ BERT Similarity\n",
    "    input_embedding = bert_model.encode([input_question], convert_to_tensor=True).view(1, -1)\n",
    "    bert_scores = util.pytorch_cos_sim(input_embedding, question_embeddings)[0].cpu().numpy()\n",
    "\n",
    "    # üîπ Combine Scores (Weighted Sum)\n",
    "    final_scores = (tfidf_scores * 0.4) + (bert_scores * 0.6)  # Adjust weights for better accuracy\n",
    "\n",
    "    # üîπ Get Top Matching Questions\n",
    "    top_indices = np.argsort(final_scores)[-top_k:][::-1]\n",
    "\n",
    "    # üîπ Retrieve Best Matching Answer\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        matched_question = df.iloc[idx][\"Question\"]\n",
    "        matched_answer = df.iloc[idx][\"Clean_Answer\"]\n",
    "        score = final_scores[idx]\n",
    "        results.append((matched_question, matched_answer, score))\n",
    "\n",
    "    return results\n",
    "\n",
    "# ‚úÖ Step 8: Test the Answer Retrieval\n",
    "input_question = \"What are the symptoms of diabetes?\"\n",
    "retrieved_answers = retrieve_best_answer(input_question, top_k=1)\n",
    "\n",
    "# ‚úÖ Step 9: Display Retrieved Answer\n",
    "print(\"\\nüîç **Input Question:**\", input_question)\n",
    "print(\"\\nüéØ **Best Matched Answer:**\")\n",
    "for idx, (matched_question, answer, score) in enumerate(retrieved_answers, start=1):\n",
    "    print(f\"{idx}. Matched Question: {matched_question} (Score: {score:.4f})\")\n",
    "    print(f\"   ‚úÖ Answer: {answer}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charupatelbaghi/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Available Columns: Index(['Question', 'Clean_Answer'], dtype='object')\n",
      "\n",
      "üì¢ Extracted Questions Sample:\n",
      "0    What is (are) Polycystic ovary syndrome ? (Als...\n",
      "1    What causes Polycystic ovary syndrome ? (Also ...\n",
      "2                        What causes Noonan syndrome ?\n",
      "3      What are the complications of Noonan syndrome ?\n",
      "4                     How to prevent Noonan syndrome ?\n",
      "5    What are the symptoms of Neurofibromatosis-Noo...\n",
      "6    Is Noonan syndrome inherited ? (Also called: M...\n",
      "7    What are the treatments for Noonan syndrome ? ...\n",
      "8    How many people are affected by polycystic kid...\n",
      "9    What are the treatments for polycystic kidney ...\n",
      "Name: Question, dtype: object\n",
      "\n",
      "üìä Question Embeddings Shape: torch.Size([1805, 384])\n",
      "\n",
      "üîç **Input Question:** What are the symptoms of diabetes?\n",
      "\n",
      "üéØ **Best Matched Answer:**\n",
      "1. Matched Question: What is (are) Diabetes ? (Score: 0.6757)\n",
      "   ‚úÖ Answer: URL: http://nihseniorhealth.gov/diabetes/toc.html\n",
      "Answer: Heart disease and stroke are the leading causes of death for people with diabetes. Controlling the ABCs of diabetes -- your blood glucose, your blood pressure, and your cholesterol, as well as stopping smoking -- can help prevent these and other complications from diabetes. -  A is for the A1C test  -  B is for Blood pressure  -  C is for Cholesterol. A is for the A1C test B is for Blood pressure C is for Cholesterol. - The A1C test (A-one-C) shows you what your blood glucose has been over the last three months. Your health care provider does this test to see what your blood glucose level is most of the time. This test should be done at least twice a year for all people with diabetes and for some people more often as needed. For many people with diabetes, an A1C test result of under 7 percent usually means that their diabetes treatment is working well and their blood glucose is under control.  The A1C test (A-one-C) shows you what your blood glucose has been over the last three months. Your health care provider does this test to see what your blood glucose level is most of the time. This test should be done at least twice a year for all people with diabetes and for some people more often as needed. For many people with diabetes, an A1C test result of under 7 percent usually means that their diabetes treatment is working well and their blood glucose is under control. -  B is for Blood pressure. The goal for most people is 140/90 but may be different for you. High blood pressure makes your heart work too hard. It can cause heart attack, stroke, and kidney disease. Your blood pressure should be checked at every doctor visit. Talk with your health care provider about your blood pressure goal.  B is for Blood pressure. The goal for most people is 140/90 but may be different for you. High blood pressure makes your heart work too hard. It can cause heart attack, stroke, and kidney disease. Your blood pressure should be checked at every doctor visit. Talk with your health care provider about your blood pressure goal. - C is for Cholesterol (ko-LES-ter-ol). The LDL goal for most people is less than 100. Low density lipoprotein, or LDL-cholesterol, is the bad cholesterol that builds up in your blood vessels. It causes the vessels to narrow and harden, which can lead to a heart attack. Your doctor should check your LDL at least once a year. Talk with your health care provider about your cholesterol goal.  C is for Cholesterol (ko-LES-ter-ol). The LDL goal for most people is less than 100. Low density lipoprotein, or LDL-cholesterol, is the bad cholesterol that builds up in your blood vessels. It causes the vessels to narrow and harden, which can lead to a heart attack. Your doctor should check your LDL at least once a year. Talk with your health care provider about your cholesterol goal. Ask your health care team - what your A1C, blood pressure, and cholesterol numbers are.  - what your ABCs should be.  - what you can do to reach your target.  what your A1C, blood pressure, and cholesterol numbers are. what your ABCs should be. what you can do to reach your target.) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:23:03.492081Z",
     "start_time": "2025-04-12T10:22:35.256733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This piece of CODE gives us three most relevant  matched questions and answers from the data set .\n",
    "# I haven't made translations into this for now!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ‚úÖ Load & Process Dataset\n",
    "file_path = \"Extracted_Medical_Q_A.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Check necessary columns\n",
    "if \"Question\" not in df.columns or \"Clean_Answer\" not in df.columns:\n",
    "    raise KeyError(\"Missing 'Question' or 'Clean_Answer' column in dataset.\")\n",
    "\n",
    "# ‚úÖ Remove Duplicates & NaNs\n",
    "df = df.dropna(subset=[\"Question\", \"Clean_Answer\"]).drop_duplicates(subset=[\"Question\"]).reset_index(drop=True)\n",
    "\n",
    "# ‚úÖ Load Pretrained BERT Model\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ‚úÖ Compute Sentence Embeddings\n",
    "question_embeddings = bert_model.encode(df[\"Question\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# ‚úÖ Check if embeddings exist\n",
    "if question_embeddings.shape[0] == 0:\n",
    "    raise ValueError(\"No question embeddings found. Check dataset processing!\")\n",
    "\n",
    "# ‚úÖ Preprocessing Functions\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def truncate_answer(answer, max_sentences=3):\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?]) +', answer.strip())\n",
    "    return ' '.join(sentences[:max_sentences])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "# ‚úÖ Train TF-IDF Model\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Question\"].apply(preprocess_text))\n",
    "\n",
    "# ‚úÖ Hybrid Retrieval\n",
    "def retrieve_best_answer(input_question, top_k=3, match_threshold=0.90):\n",
    "    processed_input = preprocess_text(input_question)\n",
    "    input_vector = vectorizer.transform([processed_input])\n",
    "    tfidf_scores = np.dot(input_vector, tfidf_matrix.T).toarray().flatten()\n",
    "    input_embedding = bert_model.encode([input_question], convert_to_tensor=True).view(1, -1)\n",
    "    bert_scores = util.pytorch_cos_sim(input_embedding, question_embeddings)[0].cpu().numpy()\n",
    "    final_scores = (tfidf_scores * 0.4) + (bert_scores * 0.6)\n",
    "    top_indices = np.argsort(final_scores)[-top_k:][::-1]\n",
    "\n",
    "    best_idx = top_indices[0]\n",
    "    best_score = final_scores[best_idx]\n",
    "    matched_question = df.iloc[best_idx][\"Question\"]\n",
    "    matched_answer = df.iloc[best_idx][\"Clean_Answer\"]\n",
    "\n",
    "    if matched_question.strip().lower() == input_question.strip().lower() or best_score >= match_threshold:\n",
    "        return [(matched_question, truncate_answer(matched_answer), best_score)]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        matched_q = df.iloc[idx][\"Question\"]\n",
    "        matched_a = df.iloc[idx][\"Clean_Answer\"]\n",
    "        score = final_scores[idx]\n",
    "        results.append((matched_q, truncate_answer(matched_a), score))\n",
    "\n",
    "    return results\n",
    "\n",
    "# ‚úÖ Example Run (Only Output Relevant Answers)\n",
    "# ‚úÖ Example Run (Take Input from User)\n",
    "while True:\n",
    "    input_question = input(\"‚ùì Enter your medical question (or type 'exit' to quit): \")\n",
    "\n",
    "    if input_question.strip().lower() == \"exit\":\n",
    "        print(\"üëã Exiting. Stay healthy!\")\n",
    "        break\n",
    "\n",
    "    retrieved_answers = retrieve_best_answer(input_question, top_k=3)\n",
    "\n",
    "    print(\"\\nüîç Top Matching Results:\\n\")\n",
    "    for idx, (matched_question, answer, score) in enumerate(retrieved_answers, start=1):\n",
    "        print(f\"{idx}. Matched Question: {matched_question} (Score: {score:.4f})\")\n",
    "        print(f\"   ‚úÖ Answer: {answer}\\n\")\n",
    "\n"
   ],
   "id": "38bcdb019416a47a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top Matching Results:\n",
      "\n",
      "1. Matched Question: Do you have information about Glucose urine test (Also called: Urine sugar test; Urine glucose test; Glucosuria test; Glycosuria test) (Score: 0.8511)\n",
      "   ‚úÖ Answer: URL: https://www.nlm.nih.gov/medlineplus/ency/article/003581.htm\n",
      "Answer: Summary : The glucose urine test measures the amount of sugar (glucose) in a urine sample. The presence of glucose in the urine is called glycosuria or glucosuria.  Glucose level can also be measured using a blood test or a cerebrospinal fluid test.\n",
      "\n",
      "How the Test is Performed : After you provide a urine sample, it is tested right away. The health care provider uses a dipstick made with a color-sensitive pad. The color the dipstick changes to tells the provider the level of glucose in your urine.   If needed,your provider may ask you to collect your urine at home over 24 hours. Your provider will tell you how to do this. Follow instructions exactly so that the results are accurate.\n",
      "\n",
      "How to Prepare for the Test : Certain medicinescan change the result of this test. Before the test, tell your provider which medicines you are taking. Do not stop taking any medicine before talking to your provider.\n",
      "\n",
      "How the Test will Feel : The test involves only normal urination. There is no discomfort.\n",
      "\n",
      "Why the Test is Performed : This test was commonly used to test for and monitor diabetes in the past. Now, blood tests to measure glucose level in the blood are easy to do and are used instead of the glucose urine test.  The glucose urine test may be ordered when the doctor suspects renal glycosuria. This is a rare condition in which glucose is released from the kidneys into the urine, even when the blood glucose level is normal.\n",
      "\n",
      "Normal Results : Glucose is not usually found in urine. If it is, further testing is needed.  Normal glucose range in urine: 0to 0.8 mmol/l (0to 15 mg/dL)  The examples above are common measurements for results of these tests. Normal value ranges may vary slightly among different laboratories. Some labs use different measurements or test different samples. Talk to yourhealth care providerabout the meaning of your specific test results.\n",
      "\n",
      "What Abnormal Results Mean : Higher than normal levels of glucose may occur with:  - Diabetes: Small increases in urine glucose levels after a large meal are not always a cause for concern.    - Pregnancy: Up to half of women have glucose in their urine at sometime during pregnancy. Glucose in the urine may mean that a woman has gestational diabetes.  - Renal glycosuria: A rare condition in which glucose is released from the kidneys into the urine, even when blood glucose levelsare normal.\n",
      "\n",
      "Risks : There are no risks with this test.) \n",
      "\n",
      "\n",
      "2. Matched Question: Do you have information about CSF glucose test (Also called: Glucose test - CSF; Cerebrospinal fluid glucose test) (Score: 0.6575)\n",
      "   ‚úÖ Answer: URL: https://www.nlm.nih.gov/medlineplus/ency/article/003633.htm\n",
      "Answer: Summary : A CSF glucose test measures the amount of sugar (glucose) in the cerebrospinal fluid (CSF). CSF is a clear fluid that flows in the space surrounding the spinal cord and brain.\n",
      "\n",
      "How the Test is Performed : A sample of CSF is needed. A lumbar puncture, also called a spinal tap, is the most common way to collect this sample. For information on this procedure, see the article on lumbar puncture.  Other methods for collecting CSF are rarely used, but may be recommended in some cases. They include:  - Cisternal puncture  - Ventricular puncture  - Removal of CSF from a tube that is already in the CSF, such as a shunt or ventricular drain    The sample is sent to a laboratory for testing.\n",
      "\n",
      "Why the Test is Performed : This test may be done to diagnose:  - Tumors  - Infections  - Inflammation of the central nervous system  - Delirium  - Other neurological and medical conditions\n",
      "\n",
      "Normal Results : The glucose level in the CSF should be 50 to 80 mg/100 mL (or greater than 2/3 of the blood sugar level).  Note: Normal value ranges may vary slightly among different laboratories. Talk to your health care provider about the meaning of your specific test results.  The examples above show the common measurements for results for these tests. Some laboratories use different measurements or may test different specimens.\n",
      "\n",
      "What Abnormal Results Mean : Abnormal results include higher and lower glucose levels. Abnormal results may be due to:  - Infection (bacterial or fungus)  - Inflammation of the central nervous system  - Tumor) \n",
      "\n",
      "\n",
      "3. Matched Question: Do you have information about Urine and Urination (Score: 0.6026)\n",
      "   ‚úÖ Answer: URL: https://www.nlm.nih.gov/medlineplus/urineandurination.html\n",
      "Answer: Summary : Your kidneys make urine by filtering wastes and extra water from your blood. The waste is called urea. Your blood carries it to the kidneys. From the kidneys, urine travels down two thin tubes called ureters to the bladder. The bladder stores urine until you are ready to urinate. It swells into a round shape when it is full and gets smaller when empty. If your urinary system is healthy, your bladder can hold up to 16 ounces (2 cups) of urine comfortably for 2 to 5 hours.    You may have problems with urination if you have       - Kidney failure    - Urinary tract infections     - An enlarged prostate    - Bladder control problems like incontinence, overactive bladder, or interstitial cystitis    - A blockage that prevents you from emptying your bladder        Some conditions may also cause you to have blood or protein in your urine. If you have a urinary problem, see your healthcare provider. Urinalysis and other urine tests can help to diagnose the problem. Treatment depends on the cause.    NIH: National Institute of Diabetes and Digestive and Kidney Diseases) \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 87\u001B[39m\n\u001B[32m     84\u001B[39m \u001B[38;5;66;03m# ‚úÖ Example Run (Only Output Relevant Answers)\u001B[39;00m\n\u001B[32m     85\u001B[39m \u001B[38;5;66;03m# ‚úÖ Example Run (Take Input from User)\u001B[39;00m\n\u001B[32m     86\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m87\u001B[39m     input_question = \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m‚ùì Enter your medical question (or type \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mexit\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m to quit): \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     89\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m input_question.strip().lower() == \u001B[33m\"\u001B[39m\u001B[33mexit\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     90\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33müëã Exiting. Stay healthy!\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001B[39m, in \u001B[36mKernel.raw_input\u001B[39m\u001B[34m(self, prompt)\u001B[39m\n\u001B[32m   1280\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1281\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[32m-> \u001B[39m\u001B[32m1282\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1283\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1284\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1285\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1286\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1287\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001B[39m, in \u001B[36mKernel._input_request\u001B[39m\u001B[34m(self, prompt, ident, parent, password)\u001B[39m\n\u001B[32m   1322\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1323\u001B[39m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[32m   1324\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInterrupted by user\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1325\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1326\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1327\u001B[39m     \u001B[38;5;28mself\u001B[39m.log.warning(\u001B[33m\"\u001B[39m\u001B[33mInvalid Message:\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T06:27:46.912669Z",
     "start_time": "2025-04-14T06:27:46.377654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from IndicTransToolkit.IndicTransToolkit import IndicProcessor\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    if qconfig == None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    translations = []\n",
    "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
    "        batch = input_sentences[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Preprocess the batch and extract entity mappings\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "        # Tokenize the batch and generate input encodings\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Generate translations using the model\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            generated_tokens = tokenizer.batch_decode(\n",
    "                generated_tokens.detach().cpu().tolist(),\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "\n",
    "        # Postprocess the translations, including entity replacement\n",
    "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ],
   "id": "dc82e640cbdba658",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Maincode + translation logic -final",
   "id": "3048d0935808fc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T06:35:51.997498Z",
     "start_time": "2025-04-14T06:27:48.921191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import scispacy\n",
    "import spacy\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# === DOWNLOAD NLTK STUFF ===\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# === LOAD NLP & MODELS ===\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# === LOAD DATASET ===\n",
    "file_path = \"Extracted_Medical_Q_A.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "if \"Question\" not in df.columns or \"Clean_Answer\" not in df.columns:\n",
    "    raise KeyError(\"Missing 'Question' or 'Clean_Answer' column in dataset.\")\n",
    "\n",
    "df = df.dropna(subset=[\"Question\", \"Clean_Answer\"]).drop_duplicates(subset=[\"Question\"]).reset_index(drop=True)\n",
    "\n",
    "# === LOAD EMBEDDINGS ===\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "question_embeddings = bert_model.encode(df[\"Question\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# === TF-IDF ===\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Question\"].apply(preprocess_text))\n",
    "\n",
    "# === TRANSLATION MODEL LOADING ===\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, None)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, None)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "# === TRANSLATION WRAPPER ===\n",
    "def translate_text(text, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    try:\n",
    "        translations = batch_translate([text], src_lang, tgt_lang, model, tokenizer, ip)\n",
    "        return translations[0] if translations else None\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# === TRUNCATE LONG ANSWERS ===\n",
    "def truncate_answer(answer, max_sentences=3):\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?]) +', answer.strip())\n",
    "    return ' '.join(sentences[:max_sentences])\n",
    "\n",
    "# === MAIN RETRIEVAL LOGIC ===\n",
    "def retrieve_best_answer(input_question_en, top_k=1, match_threshold=0.90):\n",
    "    processed_input = preprocess_text(input_question_en)\n",
    "    input_vector = vectorizer.transform([processed_input])\n",
    "    tfidf_scores = np.dot(input_vector, tfidf_matrix.T).toarray().flatten()\n",
    "    input_embedding = bert_model.encode([input_question_en], convert_to_tensor=True).view(1, -1)\n",
    "    bert_scores = util.pytorch_cos_sim(input_embedding, question_embeddings)[0].cpu().numpy()\n",
    "    final_scores = (tfidf_scores * 0.4) + (bert_scores * 0.6)\n",
    "    top_indices = np.argsort(final_scores)[-top_k:][::-1]\n",
    "\n",
    "    best_idx = top_indices[0]\n",
    "    best_score = final_scores[best_idx]\n",
    "    matched_question = df.iloc[best_idx][\"Question\"]\n",
    "    matched_answer = df.iloc[best_idx][\"Clean_Answer\"]\n",
    "\n",
    "    if matched_question.strip().lower() == input_question_en.strip().lower() or best_score >= match_threshold:\n",
    "        return [(matched_question, truncate_answer(matched_answer), best_score)]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        matched_q = df.iloc[idx][\"Question\"]\n",
    "        matched_a = df.iloc[idx][\"Clean_Answer\"]\n",
    "        score = final_scores[idx]\n",
    "        results.append((matched_q, truncate_answer(matched_a), score))\n",
    "\n",
    "    return results\n",
    "\n",
    "# === COMBINED RETRIEVAL SYSTEM: PUNJABI ENGLISH PUNJABI ===\n",
    "def punjabi_medical_qa_clir(punjabi_query, top_k=1):\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    # Step 1: Translate Punjabi to English\n",
    "    print(\"\\nüîÑ Translating Punjabi query to English...\")\n",
    "    english_query = translate_text(punjabi_query, \"pan_Guru\", \"eng_Latn\", indic_en_model, indic_en_tokenizer, ip)\n",
    "    if not english_query:\n",
    "        return [\"‚ùå Translation to English failed. Please try again.\"]\n",
    "\n",
    "    print(f\"\\nüîé Translated Query (EN): {english_query}\")\n",
    "\n",
    "    # Step 2: Retrieve top answers in English\n",
    "    print(\"\\nüì° Retrieving top answers in English...\")\n",
    "    results = retrieve_best_answer(english_query, top_k=top_k)\n",
    "\n",
    "    print(\"\\nüéØ Retrieved answers in English:\")\n",
    "    for idx, (matched_question, answer, score) in enumerate(results, 1):\n",
    "        print(f\"{idx}. {matched_question} \\nAnswer: {answer} (Score: {score:.4f})\")\n",
    "\n",
    "    # Step 3: Translate results back to Punjabi\n",
    "    print(\"\\nüîÑ Translating results back to Punjabi...\")\n",
    "    punjabi_outputs = []\n",
    "    for idx, (matched_question, answer, score) in enumerate(results, 1):\n",
    "        translated_answer = translate_text(answer, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "        if not translated_answer:\n",
    "            translated_answer = \"‚ùå Translation error while returning result.\"\n",
    "\n",
    "        punjabi_outputs.append(f\"{idx}. {translated_answer} (Score: {score:.4f})\")\n",
    "\n",
    "    return punjabi_outputs\n",
    "\n",
    "# === INTERACTIVE MODE ===\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        query = input(\"\\nü§ñ ‡®§‡©Å‡®π‡®æ‡®°‡®æ ‡®Æ‡©à‡®°‡©Ä‡®ï‡®≤ ‡®∏‡®µ‡®æ‡®≤ ‡®¶‡®∞‡®ú ‡®ï‡®∞‡©ã (Punjabi) (or type 'exit'): \").strip()\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"üëã Exiting. Stay healthy!\")\n",
    "            break\n",
    "\n",
    "        answers = punjabi_medical_qa_clir(query, top_k=3)\n",
    "        print(\"\\nüìã ‡®â‡®ö‡®ø‡®§ ‡®ú‡®µ‡®æ‡®¨ (Punjabi Summaries):\\n\")\n",
    "        for line in answers:\n",
    "            print(line)\n",
    "\n"
   ],
   "id": "6978b33d3ce66d22",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Translating Punjabi query to English...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charupatelbaghi/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Translated Query (EN): What are the symptoms of diabetes?\n",
      "\n",
      "üì° Retrieving top answers in English...\n",
      "\n",
      "üéØ Retrieved answers in English:\n",
      "1. What is (are) Diabetes ? \n",
      "Answer: URL: http://nihseniorhealth.gov/diabetes/toc.html\n",
      "Answer: Heart disease and stroke are the leading causes of death for people with diabetes. Controlling the ABCs of diabetes -- your blood glucose, your blood pressure, and your cholesterol, as well as stopping smoking -- can help prevent these and other complications from diabetes. -  A is for the A1C test  -  B is for Blood pressure  -  C is for Cholesterol. (Score: 0.6757)\n",
      "2. What are the symptoms of Diabetes ? (Also called: Diabetes - type 1; Diabetes - type 2; Diabetes - gestational; Type 1 diabetes; Type 2 diabetes; Gestational diabetes; Diabetes mellitus) \n",
      "Answer: URL: https://www.nlm.nih.gov/medlineplus/ency/article/001214.htm\n",
      "Answer: A high blood sugar level can cause several symptoms, including:  - Blurry vision  - Excess thirst  - Fatigue  - Frequent urination  - Hunger  - Weight loss    Because type 2 diabetes develops slowly, some people with high blood sugar have no symptoms. Symptoms of type 1 diabetes develop over a short period. People may be very sick by the time they are diagnosed. (Score: 0.6634)\n",
      "3. How to diagnose Diabetes ? (Also called: Diabetes - type 1; Diabetes - type 2; Diabetes - gestational; Type 1 diabetes; Type 2 diabetes; Gestational diabetes; Diabetes mellitus) \n",
      "Answer: URL: https://www.nlm.nih.gov/medlineplus/ency/article/001214.htm\n",
      "Answer: A urine analysis may show high blood sugar. But a urine test alone does not diagnose diabetes. Your health care provider may suspect that you have diabetes if your blood sugar level is higher than 200 mg/dL. (Score: 0.6132)\n",
      "\n",
      "üîÑ Translating results back to Punjabi...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 146\u001B[39m\n\u001B[32m    143\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33müëã Exiting. Stay healthy!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    144\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m146\u001B[39m answers = \u001B[43mpunjabi_medical_qa_clir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    147\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33müìã ‡®â‡®ö‡®ø‡®§ ‡®ú‡®µ‡®æ‡®¨ (Punjabi Summaries):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    148\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m answers:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 130\u001B[39m, in \u001B[36mpunjabi_medical_qa_clir\u001B[39m\u001B[34m(punjabi_query, top_k)\u001B[39m\n\u001B[32m    128\u001B[39m punjabi_outputs = []\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m idx, (matched_question, answer, score) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(results, \u001B[32m1\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m     translated_answer = \u001B[43mtranslate_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meng_Latn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpan_Guru\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43men_indic_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43men_indic_tokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    131\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m translated_answer:\n\u001B[32m    132\u001B[39m         translated_answer = \u001B[33m\"\u001B[39m\u001B[33m‚ùå Translation error while returning result.\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 67\u001B[39m, in \u001B[36mtranslate_text\u001B[39m\u001B[34m(text, src_lang, tgt_lang, model, tokenizer, ip)\u001B[39m\n\u001B[32m     65\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtranslate_text\u001B[39m(text, src_lang, tgt_lang, model, tokenizer, ip):\n\u001B[32m     66\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m         translations = \u001B[43mbatch_translate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m translations[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m translations \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     69\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 60\u001B[39m, in \u001B[36mbatch_translate\u001B[39m\u001B[34m(input_sentences, src_lang, tgt_lang, model, tokenizer, ip)\u001B[39m\n\u001B[32m     58\u001B[39m \u001B[38;5;66;03m# Generate translations using the model\u001B[39;00m\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m     generated_tokens = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     62\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     63\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmin_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m256\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[38;5;66;03m# Decode the generated tokens into text\u001B[39;00m\n\u001B[32m     71\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m tokenizer.as_target_tokenizer():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2254\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[39m\n\u001B[32m   2246\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2247\u001B[39m         input_ids=input_ids,\n\u001B[32m   2248\u001B[39m         expand_size=generation_config.num_beams,\n\u001B[32m   2249\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2250\u001B[39m         **model_kwargs,\n\u001B[32m   2251\u001B[39m     )\n\u001B[32m   2253\u001B[39m     \u001B[38;5;66;03m# 13. run beam sample\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2254\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2255\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeam_scorer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2257\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2259\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2260\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2261\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2262\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2264\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001B[32m   2265\u001B[39m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[32m   2266\u001B[39m     beam_scorer = BeamSearchScorer(\n\u001B[32m   2267\u001B[39m         batch_size=batch_size,\n\u001B[32m   2268\u001B[39m         num_beams=generation_config.num_beams,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2274\u001B[39m         max_length=generation_config.max_length,\n\u001B[32m   2275\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3463\u001B[39m, in \u001B[36mGenerationMixin._beam_search\u001B[39m\u001B[34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001B[39m\n\u001B[32m   3460\u001B[39m     outputs = stack_model_outputs(outputs_per_sub_batch, \u001B[38;5;28mself\u001B[39m.config.get_text_config())\n\u001B[32m   3462\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Unchanged original behavior\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3463\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   3465\u001B[39m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[32m   3466\u001B[39m model_kwargs = \u001B[38;5;28mself\u001B[39m._update_model_kwargs_for_generation(\n\u001B[32m   3467\u001B[39m     outputs,\n\u001B[32m   3468\u001B[39m     model_kwargs,\n\u001B[32m   3469\u001B[39m     is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   3470\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2-en-indic-1B/26ba7a4e22686dc67aab58e9d0a04cb29a1837c5/modeling_indictrans.py:1717\u001B[39m, in \u001B[36mIndicTransForConditionalGeneration.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1712\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m decoder_input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1713\u001B[39m         decoder_input_ids = shift_tokens_right(\n\u001B[32m   1714\u001B[39m             labels, \u001B[38;5;28mself\u001B[39m.config.pad_token_id, \u001B[38;5;28mself\u001B[39m.config.decoder_start_token_id\n\u001B[32m   1715\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1717\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1718\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1719\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1720\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1721\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_outputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1722\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1723\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1724\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1725\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1726\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1727\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1728\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1729\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1730\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1731\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1732\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1733\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1734\u001B[39m lm_logits = \u001B[38;5;28mself\u001B[39m.lm_head(outputs[\u001B[32m0\u001B[39m])\n\u001B[32m   1736\u001B[39m masked_lm_loss = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2-en-indic-1B/26ba7a4e22686dc67aab58e9d0a04cb29a1837c5/modeling_indictrans.py:1614\u001B[39m, in \u001B[36mIndicTransModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1607\u001B[39m     encoder_outputs = BaseModelOutput(\n\u001B[32m   1608\u001B[39m         last_hidden_state=encoder_outputs[\u001B[32m0\u001B[39m],\n\u001B[32m   1609\u001B[39m         hidden_states=encoder_outputs[\u001B[32m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(encoder_outputs) > \u001B[32m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1610\u001B[39m         attentions=encoder_outputs[\u001B[32m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(encoder_outputs) > \u001B[32m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1611\u001B[39m     )\n\u001B[32m   1613\u001B[39m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1614\u001B[39m decoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1615\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1616\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1617\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_outputs\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1618\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1619\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1620\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1621\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1622\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1623\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1624\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1625\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1626\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1629\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict:\n\u001B[32m   1630\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m decoder_outputs + encoder_outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2-en-indic-1B/26ba7a4e22686dc67aab58e9d0a04cb29a1837c5/modeling_indictrans.py:1484\u001B[39m, in \u001B[36mIndicTransDecoder.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1471\u001B[39m         layer_outputs = torch.utils.checkpoint.checkpoint(\n\u001B[32m   1472\u001B[39m             create_custom_forward(decoder_layer),\n\u001B[32m   1473\u001B[39m             hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1481\u001B[39m             \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1482\u001B[39m         )\n\u001B[32m   1483\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1484\u001B[39m         layer_outputs = \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1485\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1486\u001B[39m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1487\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1488\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1489\u001B[39m \u001B[43m            \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1490\u001B[39m \u001B[43m                \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m   1491\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1492\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcross_attn_layer_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1493\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m   1494\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m   1495\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m   1496\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1497\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1498\u001B[39m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1499\u001B[39m \u001B[43m            \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1500\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1502\u001B[39m     hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1504\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m skip_the_layer:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2-en-indic-1B/26ba7a4e22686dc67aab58e9d0a04cb29a1837c5/modeling_indictrans.py:925\u001B[39m, in \u001B[36mIndicTransDecoderLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001B[39m\n\u001B[32m    923\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.normalize_before:\n\u001B[32m    924\u001B[39m     hidden_states = \u001B[38;5;28mself\u001B[39m.final_layer_norm(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m925\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.activation_fn(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    926\u001B[39m hidden_states = F.dropout(\n\u001B[32m    927\u001B[39m     hidden_states, p=\u001B[38;5;28mself\u001B[39m.activation_dropout, training=\u001B[38;5;28mself\u001B[39m.training\n\u001B[32m    928\u001B[39m )\n\u001B[32m    929\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.fc2(hidden_states)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
<<<<<<< HEAD
=======
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "chatgpt code -optimized",
   "id": "44e462b0d5a72991"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T06:53:03.734932Z",
     "start_time": "2025-04-14T06:51:39.888203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import scispacy\n",
    "import spacy\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from IndicTransToolkit.IndicTransToolkit import IndicProcessor\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# === DOWNLOAD NLTK STUFF ===\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# === LOAD NLP & MODELS ===\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# === LOAD DATASET ===\n",
    "file_path = \"Extracted_Medical_Q_A.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "if \"Question\" not in df.columns or \"Clean_Answer\" not in df.columns:\n",
    "    raise KeyError(\"Missing 'Question' or 'Clean_Answer' column in dataset.\")\n",
    "df = df.dropna(subset=[\"Question\", \"Clean_Answer\"]).drop_duplicates(subset=[\"Question\"]).reset_index(drop=True)\n",
    "\n",
    "# === LOAD EMBEDDINGS ===\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "question_embeddings = bert_model.encode(df[\"Question\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# === TF-IDF ===\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Question\"].apply(preprocess_text))\n",
    "\n",
    "# === TRANSLATION UTILITY ===\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def initialize_model_and_tokenizer(ckpt_dir, quantization=None):\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(load_in_8bit=True, bnb_8bit_use_double_quant=True, bnb_8bit_compute_dtype=torch.bfloat16)\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    if qconfig is None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    translations = []\n",
    "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
    "        batch = input_sentences[i : i + BATCH_SIZE]\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "        inputs = tokenizer(batch, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=512,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "                early_stopping=True,\n",
    "                length_penalty=1.0\n",
    "            )\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            decoded = tokenizer.batch_decode(\n",
    "                generated.detach().cpu().tolist(),\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "\n",
    "        translations += ip.postprocess_batch(decoded, lang=tgt_lang)\n",
    "\n",
    "        del inputs, generated\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return translations\n",
    "\n",
    "def translate_paragraph(paragraph, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    translated_sentences = batch_translate(sentences, src_lang, tgt_lang, model, tokenizer, ip)\n",
    "    return \" \".join(translated_sentences)\n",
    "\n",
    "# === TRANSLATION MODEL LOADING ===\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "def translate_text(text, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    try:\n",
    "        return translate_paragraph(text, src_lang, tgt_lang, model, tokenizer, ip)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# === UTILITY ===\n",
    "def truncate_answer(answer, max_sentences=3):\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?]) +', answer.strip())\n",
    "    return ' '.join(sentences[:max_sentences])\n",
    "\n",
    "# === MAIN RETRIEVAL LOGIC ===\n",
    "def retrieve_best_answer(input_question_en, top_k=1, match_threshold=0.90):\n",
    "    processed_input = preprocess_text(input_question_en)\n",
    "    input_vector = vectorizer.transform([processed_input])\n",
    "    tfidf_scores = np.dot(input_vector, tfidf_matrix.T).toarray().flatten()\n",
    "    input_embedding = bert_model.encode([input_question_en], convert_to_tensor=True).view(1, -1)\n",
    "    bert_scores = util.pytorch_cos_sim(input_embedding, question_embeddings)[0].cpu().numpy()\n",
    "    final_scores = (tfidf_scores * 0.4) + (bert_scores * 0.6)\n",
    "    top_indices = np.argsort(final_scores)[-top_k:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        matched_q = df.iloc[idx][\"Question\"]\n",
    "        matched_a = df.iloc[idx][\"Clean_Answer\"]\n",
    "        score = final_scores[idx]\n",
    "\n",
    "        if matched_q.strip().lower() == input_question_en.strip().lower() or score >= match_threshold:\n",
    "            return [(matched_q, truncate_answer(matched_a), score)]\n",
    "\n",
    "        results.append((matched_q, truncate_answer(matched_a), score))\n",
    "\n",
    "    return results\n",
    "\n",
    "# === FULL PIPELINE ===\n",
    "def punjabi_medical_qa_clir(punjabi_query, top_k=1):\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    print(\"\\nüîÑ Translating Punjabi query to English...\")\n",
    "    english_query = translate_text(punjabi_query, \"pan_Guru\", \"eng_Latn\", indic_en_model, indic_en_tokenizer, ip)\n",
    "    if not english_query:\n",
    "        return [\"‚ùå Translation to English failed. Please try again.\"]\n",
    "\n",
    "    print(f\"\\nüîé Translated Query (EN): {english_query}\")\n",
    "    print(\"\\nüì° Retrieving top answers in English...\")\n",
    "    results = retrieve_best_answer(english_query, top_k=top_k)\n",
    "\n",
    "    print(\"\\nüéØ Retrieved answers in English:\")\n",
    "    for idx, (matched_question, answer, score) in enumerate(results, 1):\n",
    "        print(f\"{idx}. {matched_question} \\nAnswer: {answer} (Score: {score:.4f})\")\n",
    "\n",
    "    print(\"\\nüîÑ Translating results back to Punjabi...\")\n",
    "    punjabi_outputs = []\n",
    "    for idx, (matched_question, answer, score) in enumerate(results, 1):\n",
    "        translated_answer = translate_text(answer, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "        if not translated_answer:\n",
    "            translated_answer = \"‚ùå Translation error while returning result.\"\n",
    "        punjabi_outputs.append(f\"{idx}. {translated_answer} (Score: {score:.4f})\")\n",
    "\n",
    "    return punjabi_outputs\n",
    "\n",
    "# === INTERACTIVE MODE ===\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"\\nü§ñ ‡®§‡©Å‡®π‡®æ‡®°‡®æ ‡®Æ‡©à‡®°‡©Ä‡®ï‡®≤ ‡®∏‡®µ‡®æ‡®≤ ‡®¶‡®∞‡®ú ‡®ï‡®∞‡©ã (Punjabi) (or type 'exit'): \").strip()\n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"üëã Exiting. Stay healthy!\")\n",
    "\n",
    "\n",
    "    answers = punjabi_medical_qa_clir(query, top_k=1)\n",
    "    print(\"\\nüìã ‡®â‡®ö‡®ø‡®§ ‡®ú‡®µ‡®æ‡®¨ (Punjabi Summaries):\\n\")\n",
    "    for line in answers:\n",
    "        print(line)\n"
   ],
   "id": "b0c364e1dc06b254",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 199\u001B[39m\n\u001B[32m    197\u001B[39m \u001B[38;5;66;03m# === INTERACTIVE MODE ===\u001B[39;00m\n\u001B[32m    198\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m199\u001B[39m     query = \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[33;43mü§ñ ‡®§‡©Å‡®π‡®æ‡®°‡®æ ‡®Æ‡©à‡®°‡©Ä‡®ï‡®≤ ‡®∏‡®µ‡®æ‡®≤ ‡®¶‡®∞‡®ú ‡®ï‡®∞‡©ã (Punjabi) (or type \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mexit\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m): \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m.strip()\n\u001B[32m    200\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m query.lower() == \u001B[33m\"\u001B[39m\u001B[33mexit\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    201\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33müëã Exiting. Stay healthy!\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001B[39m, in \u001B[36mKernel.raw_input\u001B[39m\u001B[34m(self, prompt)\u001B[39m\n\u001B[32m   1280\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1281\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[32m-> \u001B[39m\u001B[32m1282\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1283\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1284\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1285\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1286\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1287\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001B[39m, in \u001B[36mKernel._input_request\u001B[39m\u001B[34m(self, prompt, ident, parent, password)\u001B[39m\n\u001B[32m   1322\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1323\u001B[39m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[32m   1324\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInterrupted by user\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1325\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1326\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1327\u001B[39m     \u001B[38;5;28mself\u001B[39m.log.warning(\u001B[33m\"\u001B[39m\u001B[33mInvalid Message:\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 11
>>>>>>> origin/master
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "chatgpt code -optimized",
   "id": "44e462b0d5a72991"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T08:46:38.644402Z",
     "start_time": "2025-04-14T08:40:54.934344Z"
    }
   },
   "cell_type": "code",
<<<<<<< HEAD
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import scispacy\n",
    "import spacy\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from IndicTransToolkit.IndicTransToolkit import IndicProcessor\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# === DOWNLOAD NLTK STUFF ===\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# === LOAD NLP & MODELS ===\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# === LOAD DATASET ===\n",
    "file_path = \"Extracted_Medical_Q_A.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "if \"Question\" not in df.columns or \"Clean_Answer\" not in df.columns:\n",
    "    raise KeyError(\"Missing 'Question' or 'Clean_Answer' column in dataset.\")\n",
    "df = df.dropna(subset=[\"Question\", \"Clean_Answer\"]).drop_duplicates(subset=[\"Question\"]).reset_index(drop=True)\n",
    "\n",
    "# === LOAD EMBEDDINGS ===\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "question_embeddings = bert_model.encode(df[\"Question\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# === TF-IDF ===\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Question\"].apply(preprocess_text))\n",
    "\n",
    "# === TRANSLATION UTILITY ===\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def initialize_model_and_tokenizer(ckpt_dir, quantization=None):\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(load_in_8bit=True, bnb_8bit_use_double_quant=True, bnb_8bit_compute_dtype=torch.bfloat16)\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    if qconfig is None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    translations = []\n",
    "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
    "        batch = input_sentences[i : i + BATCH_SIZE]\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "        inputs = tokenizer(batch, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=512,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "                early_stopping=True,\n",
    "                length_penalty=1.0\n",
    "            )\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            decoded = tokenizer.batch_decode(\n",
    "                generated.detach().cpu().tolist(),\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "\n",
    "        translations += ip.postprocess_batch(decoded, lang=tgt_lang)\n",
    "\n",
    "        del inputs, generated\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return translations\n",
    "\n",
    "def translate_paragraph(paragraph, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    translated_sentences = batch_translate(sentences, src_lang, tgt_lang, model, tokenizer, ip)\n",
    "    return \" \".join(translated_sentences)\n",
    "\n",
    "# === TRANSLATION MODEL LOADING ===\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "def translate_text(text, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    try:\n",
    "        return translate_paragraph(text, src_lang, tgt_lang, model, tokenizer, ip)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# === UTILITY ===\n",
    "def truncate_answer(answer, max_sentences=3):\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?]) +', answer.strip())\n",
    "    return ' '.join(sentences[:max_sentences])\n",
    "\n",
    "# === MAIN RETRIEVAL LOGIC ===\n",
    "def retrieve_best_answer(input_question_en, top_k=1, match_threshold=0.90):\n",
    "    processed_input = preprocess_text(input_question_en)\n",
    "    input_vector = vectorizer.transform([processed_input])\n",
    "    tfidf_scores = np.dot(input_vector, tfidf_matrix.T).toarray().flatten()\n",
    "    input_embedding = bert_model.encode([input_question_en], convert_to_tensor=True).view(1, -1)\n",
    "    bert_scores = util.pytorch_cos_sim(input_embedding, question_embeddings)[0].cpu().numpy()\n",
    "    final_scores = (tfidf_scores * 0.4) + (bert_scores * 0.6)\n",
    "    top_indices = np.argsort(final_scores)[-top_k:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        matched_q = df.iloc[idx][\"Question\"]\n",
    "        matched_a = df.iloc[idx][\"Clean_Answer\"]\n",
    "        score = final_scores[idx]\n",
    "\n",
    "        if matched_q.strip().lower() == input_question_en.strip().lower() or score >= match_threshold:\n",
    "            return [(matched_q, truncate_answer(matched_a), score)]\n",
    "\n",
    "        results.append((matched_q, truncate_answer(matched_a), score))\n",
    "\n",
    "    return results\n",
    "\n",
    "# === FULL PIPELINE ===\n",
    "def punjabi_medical_qa_clir(punjabi_query, top_k=1):\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    print(\"\\nüîÑ Translating Punjabi query to English...\")\n",
    "    english_query = translate_text(punjabi_query, \"pan_Guru\", \"eng_Latn\", indic_en_model, indic_en_tokenizer, ip)\n",
    "    if not english_query:\n",
    "        return [\"‚ùå Translation to English failed. Please try again.\"]\n",
    "\n",
    "    print(f\"\\nüîé Translated Query (EN): {english_query}\")\n",
    "    print(\"\\nüì° Retrieving top answers in English...\")\n",
    "    results = retrieve_best_answer(english_query, top_k=top_k)\n",
    "\n",
    "    print(\"\\nüéØ Retrieved answers in English:\")\n",
    "    for idx, (matched_question, answer, score) in enumerate(results, 1):\n",
    "        print(f\"{idx}. {matched_question} \\nAnswer: {answer} (Score: {score:.4f})\")\n",
    "\n",
    "    print(\"\\nüîÑ Translating results back to Punjabi...\")\n",
    "    punjabi_outputs = []\n",
    "    for idx, (matched_question, answer, score) in enumerate(results, 1):\n",
    "        translated_answer = translate_text(answer, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "        if not translated_answer:\n",
    "            translated_answer = \"‚ùå Translation error while returning result.\"\n",
    "        punjabi_outputs.append(f\"{idx}. {translated_answer} (Score: {score:.4f})\")\n",
    "\n",
    "    return punjabi_outputs\n",
    "\n",
    "# === INTERACTIVE MODE ===\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"\\nü§ñ ‡®§‡©Å‡®π‡®æ‡®°‡®æ ‡®Æ‡©à‡®°‡©Ä‡®ï‡®≤ ‡®∏‡®µ‡®æ‡®≤ ‡®¶‡®∞‡®ú ‡®ï‡®∞‡©ã (Punjabi) (or type 'exit'): \").strip()\n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"üëã Exiting. Stay healthy!\")\n",
    "\n",
    "\n",
    "    answers = punjabi_medical_qa_clir(query, top_k=1)\n",
    "    print(\"\\nüìã ‡®â‡®ö‡®ø‡®§ ‡®ú‡®µ‡®æ‡®¨ (Punjabi Summaries):\\n\")\n",
    "    for line in answers:\n",
    "        print(line)\n"
   ],
   "id": "b0c364e1dc06b254",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Translating Punjabi query to English...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charupatelbaghi/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Translated Query (EN): What are the symptoms of diabetes?\n",
      "\n",
      "üì° Retrieving top answers in English...\n",
      "\n",
      "üéØ Retrieved answers in English:\n",
      "1. What is (are) Diabetes ? \n",
      "Answer: URL: http://nihseniorhealth.gov/diabetes/toc.html\n",
      "Answer: Heart disease and stroke are the leading causes of death for people with diabetes. Controlling the ABCs of diabetes -- your blood glucose, your blood pressure, and your cholesterol, as well as stopping smoking -- can help prevent these and other complications from diabetes. -  A is for the A1C test  -  B is for Blood pressure  -  C is for Cholesterol. (Score: 0.6757)\n",
      "\n",
      "üîÑ Translating results back to Punjabi...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 204\u001B[39m\n\u001B[32m    200\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m query.lower() == \u001B[33m\"\u001B[39m\u001B[33mexit\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    201\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33müëã Exiting. Stay healthy!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m204\u001B[39m answers = \u001B[43mpunjabi_medical_qa_clir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    205\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33müìã ‡®â‡®ö‡®ø‡®§ ‡®ú‡®µ‡®æ‡®¨ (Punjabi Summaries):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    206\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m answers:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 190\u001B[39m, in \u001B[36mpunjabi_medical_qa_clir\u001B[39m\u001B[34m(punjabi_query, top_k)\u001B[39m\n\u001B[32m    188\u001B[39m punjabi_outputs = []\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m idx, (matched_question, answer, score) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(results, \u001B[32m1\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m     translated_answer = \u001B[43mtranslate_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meng_Latn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpan_Guru\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43men_indic_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43men_indic_tokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    191\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m translated_answer:\n\u001B[32m    192\u001B[39m         translated_answer = \u001B[33m\"\u001B[39m\u001B[33m‚ùå Translation error while returning result.\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 136\u001B[39m, in \u001B[36mtranslate_text\u001B[39m\u001B[34m(text, src_lang, tgt_lang, model, tokenizer, ip)\u001B[39m\n\u001B[32m    134\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtranslate_text\u001B[39m(text, src_lang, tgt_lang, model, tokenizer, ip):\n\u001B[32m    135\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtranslate_paragraph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    137\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    138\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTranslation Error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 120\u001B[39m, in \u001B[36mtranslate_paragraph\u001B[39m\u001B[34m(paragraph, src_lang, tgt_lang, model, tokenizer, ip)\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtranslate_paragraph\u001B[39m(paragraph, src_lang, tgt_lang, model, tokenizer, ip):\n\u001B[32m    119\u001B[39m     sentences = sent_tokenize(paragraph)\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m     translated_sentences = \u001B[43mbatch_translate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m.join(translated_sentences)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 92\u001B[39m, in \u001B[36mbatch_translate\u001B[39m\u001B[34m(input_sentences, src_lang, tgt_lang, model, tokenizer, ip)\u001B[39m\n\u001B[32m     89\u001B[39m inputs = tokenizer(batch, truncation=\u001B[38;5;28;01mTrue\u001B[39;00m, padding=\u001B[33m\"\u001B[39m\u001B[33mlongest\u001B[39m\u001B[33m\"\u001B[39m, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m).to(DEVICE)\n\u001B[32m     91\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m92\u001B[39m     generated = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     93\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     94\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmin_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     96\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m512\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     97\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     98\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     99\u001B[39m \u001B[43m        \u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    100\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlength_penalty\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1.0\u001B[39;49m\n\u001B[32m    101\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m tokenizer.as_target_tokenizer():\n\u001B[32m    104\u001B[39m     decoded = tokenizer.batch_decode(\n\u001B[32m    105\u001B[39m         generated.detach().cpu().tolist(),\n\u001B[32m    106\u001B[39m         skip_special_tokens=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    107\u001B[39m         clean_up_tokenization_spaces=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    108\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2254\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[39m\n\u001B[32m   2246\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2247\u001B[39m         input_ids=input_ids,\n\u001B[32m   2248\u001B[39m         expand_size=generation_config.num_beams,\n\u001B[32m   2249\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2250\u001B[39m         **model_kwargs,\n\u001B[32m   2251\u001B[39m     )\n\u001B[32m   2253\u001B[39m     \u001B[38;5;66;03m# 13. run beam sample\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2254\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2255\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeam_scorer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2257\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2259\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2260\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2261\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2262\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2264\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001B[32m   2265\u001B[39m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[32m   2266\u001B[39m     beam_scorer = BeamSearchScorer(\n\u001B[32m   2267\u001B[39m         batch_size=batch_size,\n\u001B[32m   2268\u001B[39m         num_beams=generation_config.num_beams,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2274\u001B[39m         max_length=generation_config.max_length,\n\u001B[32m   2275\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3463\u001B[39m, in \u001B[36mGenerationMixin._beam_search\u001B[39m\u001B[34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001B[39m\n\u001B[32m   3460\u001B[39m     outputs = stack_model_outputs(outputs_per_sub_batch, \u001B[38;5;28mself\u001B[39m.config.get_text_config())\n\u001B[32m   3462\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Unchanged original behavior\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3463\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   3465\u001B[39m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[32m   3466\u001B[39m model_kwargs = \u001B[38;5;28mself\u001B[39m._update_model_kwargs_for_generation(\n\u001B[32m   3467\u001B[39m     outputs,\n\u001B[32m   3468\u001B[39m     model_kwargs,\n\u001B[32m   3469\u001B[39m     is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   3470\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2-en-indic-1B/26ba7a4e22686dc67aab58e9d0a04cb29a1837c5/modeling_indictrans.py:1734\u001B[39m, in \u001B[36mIndicTransForConditionalGeneration.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1713\u001B[39m         decoder_input_ids = shift_tokens_right(\n\u001B[32m   1714\u001B[39m             labels, \u001B[38;5;28mself\u001B[39m.config.pad_token_id, \u001B[38;5;28mself\u001B[39m.config.decoder_start_token_id\n\u001B[32m   1715\u001B[39m         )\n\u001B[32m   1717\u001B[39m outputs = \u001B[38;5;28mself\u001B[39m.model(\n\u001B[32m   1718\u001B[39m     input_ids,\n\u001B[32m   1719\u001B[39m     attention_mask=attention_mask,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1732\u001B[39m     return_dict=return_dict,\n\u001B[32m   1733\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1734\u001B[39m lm_logits = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlm_head\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1736\u001B[39m masked_lm_loss = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1737\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1738\u001B[39m     \u001B[38;5;66;03m# move labels to the correct device to enable PP\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 17
=======
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9d681435b1554aa6"
>>>>>>> origin/master
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
