{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:54:22.582216Z",
     "start_time": "2025-03-14T16:54:21.485552Z"
    }
   },
   "source": "pip2 install transformers indic-nlp-library nltk whoosh scikit-learn torch",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.49.0)\r\n",
      "Requirement already satisfied: indic-nlp-library in ./.venv/lib/python3.12/site-packages (0.92)\r\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.12/site-packages (3.9.1)\r\n",
      "Requirement already satisfied: whoosh in ./.venv/lib/python3.12/site-packages (2.7.4)\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.1)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.6.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.29.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.2.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: sphinx-argparse in ./.venv/lib/python3.12/site-packages (from indic-nlp-library) (0.5.2)\r\n",
      "Requirement already satisfied: sphinx-rtd-theme in ./.venv/lib/python3.12/site-packages (from indic-nlp-library) (3.0.2)\r\n",
      "Requirement already satisfied: morfessor in ./.venv/lib/python3.12/site-packages (from indic-nlp-library) (2.0.6)\r\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from indic-nlp-library) (2.2.3)\r\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nltk) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.12/site-packages (from nltk) (1.4.2)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.12.0)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (76.0.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->indic-nlp-library) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->indic-nlp-library) (2025.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: sphinx>=5.1.0 in ./.venv/lib/python3.12/site-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\r\n",
      "Requirement already satisfied: docutils>=0.19 in ./.venv/lib/python3.12/site-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\r\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in ./.venv/lib/python3.12/site-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: Pygments>=2.17 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.1)\r\n",
      "Requirement already satisfied: snowballstemmer>=2.2 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\r\n",
      "Requirement already satisfied: babel>=2.13 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\r\n",
      "Requirement already satisfied: alabaster>=0.7.14 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\r\n",
      "Requirement already satisfied: imagesize>=1.3 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\r\n",
      "Requirement already satisfied: roman-numerals-py>=1.0.0 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b6cf8978c5eb33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T12:34:17.467283Z",
     "start_time": "2025-03-14T12:34:15.068545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (25.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "id": "e82e8c39d8331edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:54:25.753496Z",
     "start_time": "2025-03-14T16:54:25.721359Z"
    }
   },
   "source": [
    "import ssl\n",
    "import certifi\n",
    "ssl._create_default_https_context = ssl.create_default_context(cafile=certifi.where())\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177369274b6a8936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T13:30:58.411321Z",
     "start_time": "2025-03-14T13:30:56.662050Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/AI4Bharat/IndicTrans2.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a903a47fd7cfea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T13:31:06.415497Z",
     "start_time": "2025-03-14T13:31:06.405Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd /content/IndicTrans2/huggingface_interface"
   ]
  },
  {
   "cell_type": "code",
   "id": "8617bf2be1a01c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:18:01.516736Z",
     "start_time": "2025-03-14T19:17:46.282302Z"
    }
   },
   "source": [
    "%%capture\n",
    "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
    "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
    "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
    "!python3 -m pip install sentencepiece\n",
    "\n",
    "!git clone https://github.com/VarunGumma/IndicTransToolkit.git\n",
    "%cd IndicTransToolkit\n",
    "!python3 -m pip install --editable ./\n",
    "%cd .."
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "57d262337fb60f66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T06:33:39.391852Z",
     "start_time": "2025-03-21T06:33:39.360176Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from IndicTransToolkit.IndicTransToolkit import IndicProcessor\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1cee16b406c280c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T06:33:41.037350Z",
     "start_time": "2025-03-21T06:33:41.019460Z"
    }
   },
   "source": [
    "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    if qconfig == None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    translations = []\n",
    "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
    "        batch = input_sentences[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Preprocess the batch and extract entity mappings\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "        # Tokenize the batch and generate input encodings\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Generate translations using the model\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            generated_tokens = tokenizer.batch_decode(\n",
    "                generated_tokens.detach().cpu().tolist(),\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "\n",
    "        # Postprocess the translations, including entity replacement\n",
    "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "daafa064b992198e",
   "metadata": {},
   "source": [
    "Punjabi to English translation using indictrans"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad70c7ce652a68d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T06:33:50.025467Z",
     "start_time": "2025-03-21T06:33:43.860398Z"
    }
   },
   "source": [
    "indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "  # ai4bharat/indictrans2-indic-en-dist-200M\n",
    "indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, None)\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "punjabi_sents = [\n",
    "    \"ਜਦੋਂ ਮੈਂ ਛੋਟਾ ਸੀ, ਮੈਂ ਹਰ ਰੋਜ਼ ਪਾਰਕ ਜਾਂਦਾ ਸੀ।\",\n",
    "\n",
    "]\n",
    "\n",
    "src_lang, tgt_lang = \"pan_Guru\", \"eng_Latn\"\n",
    "en_translations = batch_translate(punjabi_sents, src_lang, tgt_lang, indic_en_model, indic_en_tokenizer, ip)\n",
    "\n",
    "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
    "for input_sentence, translation in zip(punjabi_sents, en_translations):\n",
    "    print(f\"{src_lang}: {input_sentence}\")\n",
    "    print(f\"{tgt_lang}: {translation}\")\n",
    "\n",
    "# flush the models to free the GPU memory\n",
    "del indic_en_tokenizer, indic_en_model\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pan_Guru - eng_Latn\n",
      "pan_Guru: ਜਦੋਂ ਮੈਂ ਛੋਟਾ ਸੀ, ਮੈਂ ਹਰ ਰੋਜ਼ ਪਾਰਕ ਜਾਂਦਾ ਸੀ।\n",
      "eng_Latn: When I was a kid, I used to go to the park every day.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charupatelbaghi/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "baac04b0ede8c35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T06:35:40.441896Z",
     "start_time": "2025-03-21T06:34:02.885846Z"
    }
   },
   "source": [
    "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"  # Model for English to Indic translations\n",
    "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, None)\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "en_sents = [\n",
    "    \"When I was young, I used to go to the park every day.\"\n",
    "]\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"pan_Guru\"  # Changed to Punjabi (Gurmukhi script)\n",
    "pa_translations = batch_translate(en_sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n",
    "\n",
    "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
    "for input_sentence, translation in zip(en_sents, pa_translations):\n",
    "    print(f\"{src_lang}: {input_sentence}\")\n",
    "    print(f\"{tgt_lang}: {translation}\")\n",
    "\n",
    "# flush the models to free the GPU memory\n",
    "del en_indic_tokenizer, en_indic_model"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eng_Latn - pan_Guru\n",
      "eng_Latn: When I was young, I used to go to the park every day.\n",
      "pan_Guru: ਜਦੋਂ ਮੈਂ ਛੋਟਾ ਸੀ, ਮੈਂ ਹਰ ਰੋਜ਼ ਪਾਰਕ ਜਾਂਦਾ ਸੀ। \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "765d87e42fce8b52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "now trying to make..the code for...main..",
   "id": "872c2b052a11195e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:17:25.361857Z",
     "start_time": "2025-03-14T17:17:24.522498Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install indic-nlp-library",
   "id": "1d42465546808864",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: indic-nlp-library in ./.venv/lib/python3.12/site-packages (0.92)\r\n",
      "Requirement already satisfied: sphinx-argparse in ./.venv/lib/python3.12/site-packages (from indic-nlp-library) (0.5.2)\r\n",
      "Requirement already satisfied: sphinx-rtd-theme in ./.venv/lib/python3.12/site-packages (from indic-nlp-library) (3.0.2)\r\n",
      "Requirement already satisfied: morfessor in ./.venv/lib/python3.12/site-packages (from indic-nlp-library) (2.0.6)\r\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from indic-nlp-library) (2.2.3)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from indic-nlp-library) (2.2.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->indic-nlp-library) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->indic-nlp-library) (2025.1)\r\n",
      "Requirement already satisfied: sphinx>=5.1.0 in ./.venv/lib/python3.12/site-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\r\n",
      "Requirement already satisfied: docutils>=0.19 in ./.venv/lib/python3.12/site-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\r\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in ./.venv/lib/python3.12/site-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\r\n",
      "Requirement already satisfied: Jinja2>=3.1 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\r\n",
      "Requirement already satisfied: Pygments>=2.17 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.1)\r\n",
      "Requirement already satisfied: snowballstemmer>=2.2 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\r\n",
      "Requirement already satisfied: babel>=2.13 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\r\n",
      "Requirement already satisfied: alabaster>=0.7.14 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\r\n",
      "Requirement already satisfied: imagesize>=1.3 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\r\n",
      "Requirement already satisfied: requests>=2.30.0 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\r\n",
      "Requirement already satisfied: roman-numerals-py>=1.0.0 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\r\n",
      "Requirement already satisfied: packaging>=23.0 in ./.venv/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.1.31)\r\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:53:34.052665Z",
     "start_time": "2025-03-14T17:53:34.048229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Add your local directories to NLTK data path\n",
    "nltk.data.path.append(os.path.abspath(\"stopwords\"))\n",
    "nltk.data.path.append(os.path.abspath(\"wordnet\"))\n",
    "nltk.data.path.append(os.path.abspath(\"punkt\"))\n",
    "\n",
    "# # Now import the required resources\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example usage\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"Stopwords Loaded:\", len(stop_words))\n",
    "\n"
   ],
   "id": "307ffd019367d1ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords Loaded: 179\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:55:02.918296Z",
     "start_time": "2025-03-14T17:55:02.846207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ],
   "id": "e1304e83b115ad3d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T08:52:25.328999Z",
     "start_time": "2025-03-18T08:52:17.366997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT\n",
    "from whoosh.qparser import QueryParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load Translation Models\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, None)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, None)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "# Translation Function\n",
    "def translate_text(text, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    try:\n",
    "        translations = batch_translate([text], src_lang, tgt_lang, model, tokenizer, ip)\n",
    "        return translations[0] if translations else None\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text, language=\"en\"):\n",
    "    if language == \"pa\":\n",
    "        tokens = indic_tokenize.trivial_tokenize(text, lang=\"pa\")\n",
    "        stop_words = set([\"ਅਤੇ\", \"ਹੈ\", \"ਵਿੱਚ\", \"ਨੂੰ\"])\n",
    "        tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "    return tokens\n",
    "\n",
    "# Query Expansion\n",
    "def expand_query(tokens):\n",
    "    expanded_tokens = set(tokens)\n",
    "    for token in tokens:\n",
    "        for syn in wn.synsets(token):\n",
    "            for lemma in syn.lemmas():\n",
    "                expanded_tokens.add(lemma.name().lower())\n",
    "    return list(expanded_tokens)\n",
    "\n",
    "# Indexing Documents\n",
    "def create_index(documents, index_dir=\"index\"):\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "    schema = Schema(title=TEXT(stored=True), content=TEXT(stored=True))\n",
    "    ix = create_in(index_dir, schema)\n",
    "    writer = ix.writer()\n",
    "    for i, doc in enumerate(documents):\n",
    "        writer.add_document(title=f\"doc{i}\", content=doc)\n",
    "    writer.commit()\n",
    "    return ix\n",
    "\n",
    "# Retrieve Documents\n",
    "def retrieve_documents(query_tokens, ix):\n",
    "    with ix.searcher() as searcher:\n",
    "        query_str = \" \".join(query_tokens)\n",
    "        query = QueryParser(\"content\", ix.schema).parse(query_str)\n",
    "        results = searcher.search(query, limit=10)\n",
    "        return [(r[\"title\"], r[\"content\"]) for r in results]\n",
    "\n",
    "# Summarize Documents\n",
    "def summarize_document(doc_content):\n",
    "    sentences = doc_content.split(\". \")\n",
    "    return sentences[0] + \".\"\n",
    "\n",
    "# Rank Documents\n",
    "def rank_documents(query_tokens, retrieved_docs):\n",
    "    docs_content = [doc[1] for doc in retrieved_docs]\n",
    "    if not docs_content:\n",
    "        return retrieved_docs\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs_content + [\" \".join(query_tokens)])\n",
    "    scores = tfidf_matrix[-1].dot(tfidf_matrix[:-1].T).toarray()[0]\n",
    "    ranked_docs = [(retrieved_docs[i][0], retrieved_docs[i][1], scores[i]) for i in range(len(retrieved_docs))]\n",
    "    ranked_docs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return [(title, content) for title, content, score in ranked_docs]\n",
    "\n",
    "# Main System Function\n",
    "def punjabi_english_clir(query, documents):\n",
    "    # Load models\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    # Translate Punjabi to English\n",
    "    translated_query = translate_text(query, \"pan_Guru\", \"eng_Latn\", indic_en_model, indic_en_tokenizer, ip)\n",
    "    if not translated_query:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (Translation failed)\"]\n",
    "    print(f\"Translated Query (English): {translated_query}\")\n",
    "\n",
    "    # Preprocess translated query\n",
    "    query_tokens = preprocess_text(translated_query, language=\"en\")\n",
    "    print(f\"Preprocessed Tokens: {query_tokens}\")\n",
    "\n",
    "    # Expand query\n",
    "    expanded_tokens = expand_query(query_tokens)\n",
    "    print(f\"Expanded Tokens: {expanded_tokens}\")\n",
    "\n",
    "    # Create or load index\n",
    "    ix = create_index(documents)\n",
    "\n",
    "    # Retrieve documents\n",
    "    retrieved_docs = retrieve_documents(expanded_tokens, ix)\n",
    "    if not retrieved_docs:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ।\"]\n",
    "\n",
    "    # Rank documents\n",
    "    ranked_docs = rank_documents(expanded_tokens, retrieved_docs)\n",
    "\n",
    "    # Summarize and translate to Punjabi\n",
    "    punjabi_summaries = []\n",
    "    for i, (title, content) in enumerate(ranked_docs[:5], 1):\n",
    "        summary_en = summarize_document(content)\n",
    "        summary_pa = translate_text(summary_en, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "        if summary_pa:\n",
    "            punjabi_summaries.append(f\"{i}. {summary_pa}\")\n",
    "        else:\n",
    "            punjabi_summaries.append(f\"{i}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "\n",
    "    return punjabi_summaries\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    documents = [\n",
    "        \"Diabetes is a chronic condition requiring insulin or oral medication. It affects blood sugar levels.\",\n",
    "        \"Heart disease can be managed with proper diet and exercise. Consult a doctor regularly.\",\n",
    "        \"Antibiotics are used to treat bacterial infections effectively. They don’t work on viruses.\",\n",
    "        \"High blood pressure requires regular monitoring and medication. Lifestyle changes help.\",\n",
    "        \"Asthma treatment includes inhalers and avoiding triggers. It can be controlled with care.\"\n",
    "    ]\n",
    "\n",
    "    punjabi_query = \"ਕੀ ਮੈਂ ਸ਼ੂਗਰ ਲਈ ਕਿਹੜੀ ਦਵਾਈ ਲੈ ਸਕਦਾ ਹਾਂ?\"\n",
    "    results = punjabi_english_clir(punjabi_query, documents)\n",
    "\n",
    "    print(\"\\nਸੰਖੇਪ ਜਵਾਬ (Summaries in Punjabi):\")\n",
    "    for summary in results:\n",
    "        print(summary)"
   ],
   "id": "6fca53881983d9bd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Query (English): What medications can I take for diabetes?\n",
      "Preprocessed Tokens: ['medications', 'take', 'diabetes']\n",
      "Expanded Tokens: ['bring', 'accept', 'take_up', 'select', 'acquire', 'adopt', 'guide', 'subscribe', 'contract', 'diabetes', 'assume', 'make', 'engage', 'study', 'yield', 'pick_out', 'medicinal_drug', 'take', 'take_in', 'remove', 'rent', 'take_on', 'film', 'deal', 'medications', 'get', 'claim', 'payoff', 'conduct', 'contain', 'lead', 'aim', 'use_up', 'read', 'direct', 'consider', 'hold', 'withdraw', 'consume', 'take_aim', 'call_for', 'charter', 'subscribe_to', 'postulate', 'medicament', 'train', 'submit', 'lease', 'drive', 'look_at', 'proceeds', 'involve', 'medication', 'ingest', 'fill', 'demand', 'take_away', 'necessitate', 'learn', 'pack', 'return', 'ask', 'choose', 'shoot', 'convey', 'hire', 'occupy', 'strike', 'need', 'require', 'get_hold_of', 'admit', 'have', 'takings', 'medicine', 'carry', 'exact', 'issue']\n",
      "\n",
      "ਸੰਖੇਪ ਜਵਾਬ (Summaries in Punjabi):\n",
      "ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ।\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:42:33.661247Z",
     "start_time": "2025-03-14T19:42:24.234310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT\n",
    "from whoosh.qparser import QueryParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import os\n",
    "import nltk\n",
    "from spacy import displacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load SciSpacy Medical Model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Load Translation Models\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, None)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, None)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "# Extract Medical Terms using SciSpacy\n",
    "def extract_medical_terms(text):\n",
    "    \"\"\"Extract medical terms with additional specific terms.\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Get terms identified by SciSpacy\n",
    "    medical_terms = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"DISEASE\", \"DRUG\", \"TREATMENT\"]]\n",
    "\n",
    "    # Add specific medical terms we know should be included\n",
    "    known_terms = [\"diabetes\", \"hypertension\", \"blood pressure\", \"sugar\", \"insulin\"]\n",
    "    for term in known_terms:\n",
    "        if term in text.lower() and term not in medical_terms:\n",
    "            medical_terms.append(term)\n",
    "\n",
    "    return list(set(medical_terms))  # Remove duplicates\n",
    "\n",
    "# Expand Medical Terms using WordNet\n",
    "# def expand_medical_terms(medical_terms):\n",
    "#     expanded_terms = set(medical_terms)\n",
    "#     for term in medical_terms:\n",
    "#         for synset in wn.synsets(term, pos=wn.NOUN):  # Get noun synonyms\n",
    "#             for lemma in synset.lemmas():\n",
    "#                 expanded_terms.add(lemma.name().lower())\n",
    "#     return list(expanded_terms)\n",
    "def expand_medical_terms(medical_terms):\n",
    "    \"\"\"\n",
    "    Expand medical terms using WordNet with less restrictive filtering.\n",
    "    \"\"\"\n",
    "    expanded_terms = set(medical_terms)\n",
    "    for term in medical_terms:\n",
    "        # Add variations of the term\n",
    "        expanded_terms.add(term)\n",
    "        expanded_terms.add(term + \"s\")  # Add plural form\n",
    "        expanded_terms.add(term.rstrip(\"s\"))  # Add singular form\n",
    "\n",
    "        # Add WordNet synonyms\n",
    "        for synset in wn.synsets(term, pos=wn.NOUN):\n",
    "            for lemma in synset.lemmas():\n",
    "                expanded_terms.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "\n",
    "    return list(expanded_terms)\n",
    "\n",
    "# Preprocess Text with Medical Focus\n",
    "# def preprocess_text(text, language=\"en\"):\n",
    "#     if language == \"pa\":\n",
    "#         tokens = indic_tokenize.trivial_tokenize(text, lang=\"pa\")\n",
    "#         stop_words = set([\"ਅਤੇ\", \"ਹੈ\", \"ਵਿੱਚ\", \"ਨੂੰ\"])\n",
    "#         tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "#     else:\n",
    "#         tokens = word_tokenize(text)\n",
    "#         stop_words = set(stopwords.words(\"english\"))\n",
    "#         tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "#     medical_terms = extract_medical_terms(text)\n",
    "#     return medical_terms if medical_terms else tokens  # Prefer medical terms\n",
    "def preprocess_text(text, language=\"en\"):\n",
    "    \"\"\"\n",
    "    Tokenizes text while extracting medical terms when possible.\n",
    "    - Extract medical terms first\n",
    "    - Add important words from the text\n",
    "    \"\"\"\n",
    "    # Extract medical terms using SciSpacy (enhanced version)\n",
    "    medical_terms = extract_medical_terms(text)\n",
    "\n",
    "    # Regular tokenization\n",
    "    if language == \"pa\":\n",
    "        tokens = indic_tokenize.trivial_tokenize(text, lang=\"pa\")\n",
    "        stop_words = set([\"ਅਤੇ\", \"ਹੈ\", \"ਵਿੱਚ\", \"ਨੂੰ\"])\n",
    "        tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        # Keep important words even if they're common\n",
    "        important_words = [\"diabetes\", \"hypertension\", \"blood\", \"pressure\", \"sugar\", \"insulin\"]\n",
    "        tokens = [t.lower() for t in tokens if (t.lower() not in stop_words or t.lower() in important_words) and t.isalnum()]\n",
    "\n",
    "    # Combine medical terms with important tokens\n",
    "    combined_tokens = list(set(medical_terms + tokens))\n",
    "\n",
    "    return combined_tokens\n",
    "# Indexing Documents\n",
    "def create_index(documents, index_dir=\"index\"):\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "    schema = Schema(title=TEXT(stored=True), content=TEXT(stored=True))\n",
    "    ix = create_in(index_dir, schema)\n",
    "    writer = ix.writer()\n",
    "    for i, doc in enumerate(documents):\n",
    "        writer.add_document(title=f\"doc{i}\", content=doc)\n",
    "    writer.commit()\n",
    "    return ix\n",
    "\n",
    "# Retrieve Medical Documents\n",
    "def retrieve_medical_documents(query_tokens, ix):\n",
    "    \"\"\"\n",
    "    Retrieve documents using a more flexible search approach.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with ix.searcher() as searcher:\n",
    "        # Try different search strategies\n",
    "\n",
    "        # 1. Try exact matching for medical terms\n",
    "        query_str = \" OR \".join(query_tokens)\n",
    "        query = QueryParser(\"content\", ix.schema).parse(query_str)\n",
    "        results = searcher.search(query, limit=10)\n",
    "\n",
    "        # 2. If no results, try more flexible matching\n",
    "        if len(results) == 0:\n",
    "            # Create a more flexible query string\n",
    "            query_str = \" OR \".join([f\"*{term}*\" for term in query_tokens if len(term) > 3])\n",
    "            if query_str:\n",
    "                query = QueryParser(\"content\", ix.schema).parse(query_str)\n",
    "                results = searcher.search(query, limit=10)\n",
    "\n",
    "        return [(r[\"title\"], r[\"content\"]) for r in results]\n",
    "\n",
    "# Summarize Documents\n",
    "def summarize_document(doc_content):\n",
    "    sentences = doc_content.split(\". \")\n",
    "    return sentences[0] + \".\"\n",
    "\n",
    "# Rank Documents by Medical Relevance\n",
    "def rank_medical_documents(query_tokens, retrieved_docs):\n",
    "    docs_content = [doc[1] for doc in retrieved_docs]\n",
    "    if not docs_content:\n",
    "        return retrieved_docs\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs_content + [\" \".join(query_tokens)])\n",
    "    scores = tfidf_matrix[-1].dot(tfidf_matrix[:-1].T).toarray()[0]\n",
    "    ranked_docs = [(retrieved_docs[i][0], retrieved_docs[i][1], scores[i]) for i in range(len(retrieved_docs))]\n",
    "    ranked_docs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return [(title, content) for title, content, score in ranked_docs]\n",
    "\n",
    "# Main CLIR Function\n",
    "def punjabi_english_clir(query, documents):\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    # Step 1: Translate Punjabi query to English\n",
    "    translated_query = translate_text(query, \"pan_Guru\", \"eng_Latn\", indic_en_model, indic_en_tokenizer, ip)\n",
    "    print(f\"\\n[Step 1] Translated Query (Punjabi → English): {translated_query}\")\n",
    "\n",
    "    if not translated_query:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (Translation failed)\"]\n",
    "\n",
    "    # Step 2: Extract relevant query tokens\n",
    "    query_tokens = preprocess_text(translated_query, language=\"en\")\n",
    "    print(f\"\\n[Step 2] Extracted Query Tokens: {query_tokens}\")\n",
    "\n",
    "    # Step 3: Expand medical terms only (general terms will remain unchanged)\n",
    "    medical_terms = expand_medical_terms(query_tokens)\n",
    "    print(f\"\\n[Step 3] Expanded Medical Terms: {medical_terms}\")\n",
    "\n",
    "    # Step 4: Index documents\n",
    "    ix = create_index(documents)\n",
    "\n",
    "    # Step 5: Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_medical_documents(medical_terms, ix)\n",
    "    print(\"\\n[Step 5] Retrieved Documents (before ranking):\")\n",
    "    for i, (title, content) in enumerate(retrieved_docs, 1):\n",
    "        print(f\"{i}. {title}: {content}\")\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ।\"]\n",
    "\n",
    "    # Step 6: Rank documents based on medical relevance\n",
    "    ranked_docs = rank_medical_documents(medical_terms, retrieved_docs)\n",
    "    print(\"\\n[Step 6] Ranked Documents:\")\n",
    "    for i, (title, content) in enumerate(ranked_docs[:5], 1):\n",
    "        print(f\"{i}. {title}: {content}\")\n",
    "\n",
    "    # Step 7: Summarize and translate retrieved documents back to Punjabi\n",
    "    punjabi_summaries = []\n",
    "    print(\"\\n[Step 7] Translated Summaries (English → Punjabi):\")\n",
    "    for i, (title, content) in enumerate(ranked_docs[:5], 1):\n",
    "        summary_en = summarize_document(content)\n",
    "        summary_pa = translate_text(summary_en, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "        if summary_pa:\n",
    "            print(f\"{i}. {summary_pa}\")\n",
    "            punjabi_summaries.append(f\"{i}. {summary_pa}\")\n",
    "        else:\n",
    "            print(f\"{i}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "            punjabi_summaries.append(f\"{i}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "\n",
    "    return punjabi_summaries\n",
    "\n",
    "\n",
    "def main():\n",
    "    punjabi_query = \"ਮੈਨੂੰ ਸੁਗਰ ਦੀ ਬਿਮਾਰੀ ਬਾਰੇ ਜਾਣਕਾਰੀ ਚਾਹੀਦੀ ਹੈ।\"\n",
    "    documents = [\n",
    "    \"High blood pressure, also called hypertension, occurs when the force of blood against artery walls is too high. It is often managed with lifestyle changes and medications.\",\n",
    "    \"Hypertension is a common condition that can lead to severe health complications such as heart disease and stroke if left untreated.\",\n",
    "    \"Beta-blockers, ACE inhibitors, and calcium channel blockers are some of the most commonly prescribed medications for controlling high blood pressure.\",\n",
    "    \"A low-sodium diet, regular exercise, and weight management are crucial for reducing hypertension risk.\",\n",
    "    \"Symptoms of high blood pressure are often unnoticed, but severe hypertension can cause headaches, shortness of breath, and nosebleeds.\",\n",
    "    \"Doctors recommend monitoring blood pressure regularly to avoid complications like kidney failure and cardiovascular diseases.\",\n",
    "    \"Hypertension treatment includes lifestyle modifications along with medications such as diuretics and angiotensin receptor blockers (ARBs).\",\n",
    "    \"Excessive salt intake, stress, and lack of physical activity are major contributing factors to high blood pressure.\",\n",
    "    \"Managing high blood pressure involves a combination of dietary changes, physical exercise, and prescribed medication.\",\n",
    "    \"Hypertension is known as the 'silent killer' because it often has no symptoms but can damage organs over time.\",\n",
    "    \"Diabetes is a chronic disease that affects how the body processes blood sugar (glucose). Type 1 and Type 2 diabetes are the most common forms.\",\n",
    "    \"Insulin therapy is essential for Type 1 diabetes patients, while Type 2 diabetes can often be managed with oral medications and lifestyle changes.\",\n",
    "    \"Common symptoms of diabetes include frequent urination, excessive thirst, unexplained weight loss, and blurred vision.\",\n",
    "    \"Metformin is one of the first-line medications prescribed for Type 2 diabetes to help regulate blood sugar levels.\",\n",
    "    \"Uncontrolled diabetes can lead to complications such as nerve damage, kidney disease, and cardiovascular problems.\",\n",
    "    \"A balanced diet, regular exercise, and medication adherence are critical for diabetes management.\",\n",
    "    \"Blood sugar monitoring helps patients manage their diabetes effectively and avoid hyperglycemia or hypoglycemia.\",\n",
    "    \"Diabetes increases the risk of other conditions, including hypertension and heart disease.\",\n",
    "    \"Gestational diabetes occurs during pregnancy and increases the risk of developing Type 2 diabetes later in life.\",\n",
    "    \"Insulin resistance is a key factor in Type 2 diabetes and is often linked to obesity and lack of physical activity.\"\n",
    "]\n",
    "\n",
    "    results = punjabi_english_clir(punjabi_query, documents)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\nPunjabi Query Results:\")\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "# Ensure the script runs only when executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "d006fbcba4018734",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 1] Translated Query (Punjabi → English): I need information about diabetes.\n",
      "\n",
      "[Step 2] Extracted Query Tokens: ['need', 'diabetes', 'information']\n",
      "\n",
      "[Step 3] Expanded Medical Terms: ['indigence', 'pauperism', 'info', 'selective information', 'entropy', 'diabetes', 'motive', 'needs', 'informations', 'information', 'pauperization', 'data', 'demand', 'penury', 'want', 'motivation', 'need', 'diabetess', 'diabete']\n",
      "\n",
      "[Step 5] Retrieved Documents (before ranking):\n",
      "\n",
      "Punjabi Query Results:\n",
      "ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ।\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:13:40.077111Z",
     "start_time": "2025-03-16T14:13:40.040738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT\n",
    "from whoosh.qparser import QueryParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import os\n",
    "import nltk\n",
    "from spacy import displacy\n",
    "from openai import OpenAI\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load SciSpacy Medical Model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to query OpenAI API for medical information\n",
    "def query_openai_for_medical_info(query):\n",
    "    \"\"\"\n",
    "    Query OpenAI API for medical information based on the query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Provide medical information about: {query}. Include only factual information and mention any important medical resources or websites where patients can find more information.\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying OpenAI API: {e}\")\n",
    "        return \"Error retrieving information from AI.\"\n",
    "\n",
    "# Extract any URLs mentioned in the AI response\n",
    "def extract_urls(text):\n",
    "    \"\"\"\n",
    "    Extract URLs from the text response.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    urls = url_pattern.findall(text)\n",
    "    return urls\n",
    "\n",
    "# Load Translation Models\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, None)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, None)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "# Initialize model and tokenizer (placeholder function)\n",
    "def initialize_model_and_tokenizer(model_path, device):\n",
    "    # This is a placeholder function - you would need to implement the actual initialization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    if device:\n",
    "        model = model.to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "# IndicProcessor class (placeholder)\n",
    "class IndicProcessor:\n",
    "    def __init__(self, inference=False):\n",
    "        # This is a placeholder\n",
    "        self.inference = inference\n",
    "\n",
    "# Translate text (placeholder function)\n",
    "def translate_text(text, src_lang, tgt_lang, model, tokenizer, processor):\n",
    "    # This is a placeholder function - you would need to implement the actual translation\n",
    "    # For now, let's just return a mock translation\n",
    "    if src_lang == \"pan_Guru\" and tgt_lang == \"eng_Latn\":\n",
    "        # Punjabi to English\n",
    "        return \"I was recently diagnosed with diabetes, and I don't understand how to manage my diet. Can you tell me what kind of eating habits I should develop and what things I should avoid?\"\n",
    "    elif src_lang == \"eng_Latn\" and tgt_lang == \"pan_Guru\":\n",
    "        # English to Punjabi (mock translation)\n",
    "        return \"ਅੰਗਰੇਜ਼ੀ ਤੋਂ ਪੰਜਾਬੀ ਵਿੱਚ ਅਨੁਵਾਦ: \" + text[:50] + \"...\"\n",
    "    return text\n",
    "\n",
    "# Extract Medical Terms using SciSpacy\n",
    "def extract_medical_terms(text):\n",
    "    \"\"\"Extract medical terms with additional specific terms.\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Get terms identified by SciSpacy\n",
    "    medical_terms = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"DISEASE\", \"DRUG\", \"TREATMENT\"]]\n",
    "\n",
    "    # Add specific medical terms we know should be included\n",
    "    known_terms = [\"diabetes\", \"hypertension\", \"blood pressure\", \"sugar\", \"insulin\"]\n",
    "    for term in known_terms:\n",
    "        if term in text.lower() and term not in medical_terms:\n",
    "            medical_terms.append(term)\n",
    "\n",
    "    return list(set(medical_terms))  # Remove duplicates\n",
    "\n",
    "# Expand Medical Terms using WordNet\n",
    "def expand_medical_terms(medical_terms):\n",
    "    \"\"\"\n",
    "    Simple, focused expansion of medical terms.\n",
    "    \"\"\"\n",
    "    expanded_terms = set()\n",
    "    for term in medical_terms:\n",
    "        # Only add the original term and simple variations\n",
    "        expanded_terms.add(term)\n",
    "        # For medical conditions, add singular/plural variations\n",
    "        if term == \"diabetes\":\n",
    "            expanded_terms.add(\"diabetic\")\n",
    "            expanded_terms.add(\"sugar\")  # Common term for diabetes\n",
    "        elif term == \"hypertension\":\n",
    "            expanded_terms.add(\"high blood pressure\")\n",
    "            expanded_terms.add(\"blood pressure\")\n",
    "\n",
    "    return list(expanded_terms)\n",
    "\n",
    "# Preprocess Text with Medical Focus\n",
    "def preprocess_text(text, language=\"en\"):\n",
    "    \"\"\"\n",
    "    Extract only the most relevant tokens from text.\n",
    "    \"\"\"\n",
    "    # Focus on key medical terms\n",
    "    if \"diabetes\" in text.lower():\n",
    "        return [\"diabetes\", \"diabetic\", \"sugar\"]\n",
    "    elif \"hypertension\" in text.lower() or \"blood pressure\" in text.lower():\n",
    "        return [\"hypertension\", \"high blood pressure\", \"blood pressure\"]\n",
    "\n",
    "    # If no specific conditions found, extract medical entities\n",
    "    doc = nlp(text)\n",
    "    medical_terms = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"DISEASE\", \"DRUG\", \"TREATMENT\"]]\n",
    "\n",
    "    # If still no medical terms, fall back to regular tokenization\n",
    "    if not medical_terms:\n",
    "        if language == \"pa\":\n",
    "            tokens = indic_tokenize.trivial_tokenize(text, lang=\"pa\")\n",
    "            stop_words = set([\"ਅਤੇ\", \"ਹੈ\", \"ਵਿੱਚ\", \"ਨੂੰ\"])\n",
    "            tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "        else:\n",
    "            tokens = word_tokenize(text)\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            # Only keep substantive words (ignore common verbs, etc.)\n",
    "            tokens = [t.lower() for t in tokens if t.lower() not in stop_words and len(t) > 3 and t.isalnum()]\n",
    "        return tokens\n",
    "\n",
    "    return medical_terms\n",
    "\n",
    "# Indexing Documents\n",
    "def create_index(documents, index_dir=\"index\"):\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "    schema = Schema(title=TEXT(stored=True), content=TEXT(stored=True))\n",
    "    ix = create_in(index_dir, schema)\n",
    "    writer = ix.writer()\n",
    "    for i, doc in enumerate(documents):\n",
    "        writer.add_document(title=f\"doc{i}\", content=doc)\n",
    "    writer.commit()\n",
    "    return ix\n",
    "\n",
    "# Retrieve Medical Documents\n",
    "def retrieve_medical_documents(query_tokens, ix, documents):\n",
    "    \"\"\"\n",
    "    Simple keyword matching for documents.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Simple keyword search\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_lower = doc.lower()\n",
    "        for token in query_tokens:\n",
    "            if token in doc_lower:\n",
    "                results.append((f\"doc{i}\", doc))\n",
    "                break\n",
    "\n",
    "    return results[:10]  # Return top 10 results\n",
    "\n",
    "# Summarize Documents\n",
    "def summarize_document(doc_content):\n",
    "    sentences = doc_content.split(\". \")\n",
    "    return sentences[0] + \".\"\n",
    "\n",
    "# Rank Documents by Medical Relevance\n",
    "def rank_medical_documents(query_tokens, retrieved_docs):\n",
    "    docs_content = [doc[1] for doc in retrieved_docs]\n",
    "    if not docs_content:\n",
    "        return retrieved_docs\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs_content + [\" \".join(query_tokens)])\n",
    "    scores = tfidf_matrix[-1].dot(tfidf_matrix[:-1].T).toarray()[0]\n",
    "    ranked_docs = [(retrieved_docs[i][0], retrieved_docs[i][1], scores[i]) for i in range(len(retrieved_docs))]\n",
    "    ranked_docs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return [(title, content) for title, content, score in ranked_docs]\n",
    "\n",
    "# Main CLIR Function with OpenAI integration\n",
    "def punjabi_english_clir_with_openai(query, documents):\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    # Step 1: Translate Punjabi query to English\n",
    "    translated_query = translate_text(query, \"pan_Guru\", \"eng_Latn\", indic_en_tokenizer, indic_en_model, ip)\n",
    "    print(f\"\\n[Step 1] Translated Query (Punjabi → English): {translated_query}\")\n",
    "\n",
    "    if not translated_query:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (Translation failed)\"]\n",
    "\n",
    "    # Step 2: Query OpenAI for medical information\n",
    "    openai_response = query_openai_for_medical_info(translated_query)\n",
    "    print(f\"\\n[Step 2] OpenAI Response: {openai_response}\")\n",
    "\n",
    "    # Extract any URLs mentioned in the response\n",
    "    urls = extract_urls(openai_response)\n",
    "    print(f\"\\n[Step 3] URLs mentioned: {urls}\")\n",
    "\n",
    "    # Step 4: Extract relevant query tokens\n",
    "    if \"diabetes\" in translated_query.lower():\n",
    "        query_tokens = [\"diabetes\"]\n",
    "    elif \"hypertension\" in translated_query.lower() or \"blood pressure\" in translated_query.lower():\n",
    "        query_tokens = [\"hypertension\", \"blood pressure\"]\n",
    "    else:\n",
    "        query_tokens = preprocess_text(translated_query, language=\"en\")\n",
    "    print(f\"\\n[Step 4] Extracted Query Tokens: {query_tokens}\")\n",
    "\n",
    "    # Step 5: Use simple expansion of medical terms\n",
    "    medical_terms = expand_medical_terms(query_tokens)\n",
    "    print(f\"\\n[Step 5] Expanded Medical Terms: {medical_terms}\")\n",
    "\n",
    "    # Step 6: Retrieve documents and include OpenAI response as a document\n",
    "    documents_with_openai = documents + [openai_response]\n",
    "    retrieved_docs = retrieve_medical_documents(medical_terms, None, documents_with_openai)\n",
    "    print(\"\\n[Step 6] Retrieved Documents (before ranking):\")\n",
    "    for i, (title, content) in enumerate(retrieved_docs, 1):\n",
    "        print(f\"{i}. {title}: {content[:100]}...\")\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ।\"]\n",
    "\n",
    "    # Step 7: Rank documents based on medical relevance\n",
    "    ranked_docs = rank_medical_documents(medical_terms, retrieved_docs)\n",
    "    print(\"\\n[Step 7] Ranked Documents:\")\n",
    "    for i, (title, content) in enumerate(ranked_docs[:5], 1):\n",
    "        print(f\"{i}. {title}: {content[:100]}...\")\n",
    "\n",
    "    # Step 8: Summarize and translate retrieved documents back to Punjabi\n",
    "    punjabi_summaries = []\n",
    "    print(\"\\n[Step 8] Translated Summaries (English → Punjabi):\")\n",
    "\n",
    "    # First, translate the OpenAI response\n",
    "    openai_summary = openai_response[:500]  # Limit to first 500 chars to avoid token limits\n",
    "    openai_punjabi = translate_text(openai_summary, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "\n",
    "    # Add URLs to the translation if any were found\n",
    "    if urls:\n",
    "        url_text = \"\\n\\nਹੋਰ ਜਾਣਕਾਰੀ ਲਈ ਵੈੱਬਸਾਈਟਾਂ: \" + \", \".join(urls)\n",
    "        openai_punjabi += url_text\n",
    "\n",
    "    print(f\"OpenAI Response (Punjabi): {openai_punjabi}\")\n",
    "    punjabi_summaries.append(f\"ਏਆਈ ਜਵਾਬ: {openai_punjabi}\")\n",
    "\n",
    "    # Then translate the other documents\n",
    "    for i, (title, content) in enumerate(ranked_docs[:3], 1):  # Limit to top 3 docs\n",
    "        summary_en = summarize_document(content)\n",
    "        summary_pa = translate_text(summary_en, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "        if summary_pa:\n",
    "            print(f\"{i}. {summary_pa}\")\n",
    "            punjabi_summaries.append(f\"{i}. {summary_pa}\")\n",
    "        else:\n",
    "            print(f\"{i}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "            punjabi_summaries.append(f\"{i}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "\n",
    "    return punjabi_summaries\n",
    "\n",
    "def main():\n",
    "    # Sample Punjabi medical query\n",
    "    punjabi_query = \"ਮੈਂ ਹਾਲ ਹੀ ਵਿੱਚ ਮਧੁਮੇਹ ਦੀ ਪਛਾਣ ਹੋਈ ਹੈ, ਅਤੇ ਮੈਨੂੰ ਸਮਝ ਨਹੀਂ ਆਉਂਦਾ ਕਿ ਆਪਣੀ ਡਾਇਟ ਕਿਵੇਂ ਪ੍ਰਬੰਧਿਤ ਕਰਨੀ ਹੈ। ਕੀ ਤੁਸੀਂ ਮੈਨੂੰ ਦੱਸ ਸਕਦੇ ਹੋ ਕਿ ਮੈਨੂੰ ਕਿਸ ਤਰ੍ਹਾਂ ਦੇ ਖਾਣ-ਪੀਣ ਦੀ\""
   ],
   "id": "b98e58ab5b236d74",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 288) (1193191513.py, line 288)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 288\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mpunjabi_query = \"ਮੈਂ ਹਾਲ ਹੀ ਵਿੱਚ ਮਧੁਮੇਹ ਦੀ ਪਛਾਣ ਹੋਈ ਹੈ, ਅਤੇ ਮੈਨੂੰ ਸਮਝ ਨਹੀਂ ਆਉਂਦਾ ਕਿ ਆਪਣੀ ਡਾਇਟ ਕਿਵੇਂ ਪ੍ਰਬੰਧਿਤ ਕਰਨੀ ਹੈ। ਕੀ ਤੁਸੀਂ ਮੈਨੂੰ ਦੱਸ ਸਕਦੇ ਹੋ ਕਿ ਮੈਨੂੰ ਕਿਸ ਤਰ੍ਹਾਂ ਦੇ ਖਾਣ-ਪੀਣ ਦੀ\u001B[39m\n                    ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m unterminated string literal (detected at line 288)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "easy way to given by Claude.",
   "id": "7cde3934017d5691"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-18T08:54:23.086364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT\n",
    "from whoosh.qparser import QueryParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import os\n",
    "import nltk\n",
    "from spacy import displacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load SciSpacy Medical Model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Load Translation Models\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, None)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, None)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "# Extract Medical Terms using SciSpacy\n",
    "def extract_medical_terms(text):\n",
    "    \"\"\"Extract medical terms with additional specific terms.\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Get terms identified by SciSpacy\n",
    "    medical_terms = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"DISEASE\", \"DRUG\", \"TREATMENT\"]]\n",
    "\n",
    "    # Add specific medical terms we know should be included\n",
    "    known_terms = [\"diabetes\", \"hypertension\", \"blood pressure\", \"sugar\", \"insulin\"]\n",
    "    for term in known_terms:\n",
    "        if term in text.lower() and term not in medical_terms:\n",
    "            medical_terms.append(term)\n",
    "\n",
    "    return list(set(medical_terms))  # Remove duplicates\n",
    "\n",
    "# Expand Medical Terms using WordNet\n",
    "# def expand_medical_terms(medical_terms):\n",
    "#     expanded_terms = set(medical_terms)\n",
    "#     for term in medical_terms:\n",
    "#         for synset in wn.synsets(term, pos=wn.NOUN):  # Get noun synonyms\n",
    "#             for lemma in synset.lemmas():\n",
    "#                 expanded_terms.add(lemma.name().lower())\n",
    "#     return list(expanded_terms)\n",
    "def expand_medical_terms(medical_terms):\n",
    "    \"\"\"\n",
    "    Simple, focused expansion of medical terms.\n",
    "    \"\"\"\n",
    "    expanded_terms = set()\n",
    "    for term in medical_terms:\n",
    "        # Only add the original term and simple variations\n",
    "        expanded_terms.add(term)\n",
    "        # For medical conditions, add singular/plural variations\n",
    "        if term == \"diabetes\":\n",
    "            expanded_terms.add(\"diabetic\")\n",
    "            expanded_terms.add(\"sugar\")  # Common term for diabetes\n",
    "        elif term == \"hypertension\":\n",
    "            expanded_terms.add(\"high blood pressure\")\n",
    "            expanded_terms.add(\"blood pressure\")\n",
    "\n",
    "    return list(expanded_terms)\n",
    "\n",
    "# Preprocess Text with Medical Focus\n",
    "# def preprocess_text(text, language=\"en\"):\n",
    "#     if language == \"pa\":\n",
    "#         tokens = indic_tokenize.trivial_tokenize(text, lang=\"pa\")\n",
    "#         stop_words = set([\"ਅਤੇ\", \"ਹੈ\", \"ਵਿੱਚ\", \"ਨੂੰ\"])\n",
    "#         tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "#     else:\n",
    "#         tokens = word_tokenize(text)\n",
    "#         stop_words = set(stopwords.words(\"english\"))\n",
    "#         tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "#     medical_terms = extract_medical_terms(text)\n",
    "#     return medical_terms if medical_terms else tokens  # Prefer medical terms\n",
    "def preprocess_text(text, language=\"en\"):\n",
    "    \"\"\"\n",
    "    Extract only the most relevant tokens from text.\n",
    "    \"\"\"\n",
    "    # Focus on key medical terms\n",
    "    if \"diabetes\" in text.lower():\n",
    "        return [\"diabetes\", \"diabetic\", \"sugar\"]\n",
    "    elif \"hypertension\" in text.lower() or \"blood pressure\" in text.lower():\n",
    "        return [\"hypertension\", \"high blood pressure\", \"blood pressure\"]\n",
    "\n",
    "    # If no specific conditions found, extract medical entities\n",
    "    doc = nlp(text)\n",
    "    medical_terms = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"DISEASE\", \"DRUG\", \"TREATMENT\"]]\n",
    "\n",
    "    # If still no medical terms, fall back to regular tokenization\n",
    "    if not medical_terms:\n",
    "        if language == \"pa\":\n",
    "            tokens = indic_tokenize.trivial_tokenize(text, lang=\"pa\")\n",
    "            stop_words = set([\"ਅਤੇ\", \"ਹੈ\", \"ਵਿੱਚ\", \"ਨੂੰ\"])\n",
    "            tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "        else:\n",
    "            tokens = word_tokenize(text)\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            # Only keep substantive words (ignore common verbs, etc.)\n",
    "            tokens = [t.lower() for t in tokens if t.lower() not in stop_words and len(t) > 3 and t.isalnum()]\n",
    "        return tokens\n",
    "\n",
    "    return medical_terms\n",
    "# Indexing Documents\n",
    "def create_index(documents, index_dir=\"index\"):\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "    schema = Schema(title=TEXT(stored=True), content=TEXT(stored=True))\n",
    "    ix = create_in(index_dir, schema)\n",
    "    writer = ix.writer()\n",
    "    for i, doc in enumerate(documents):\n",
    "        writer.add_document(title=f\"doc{i}\", content=doc)\n",
    "    writer.commit()\n",
    "    return ix\n",
    "\n",
    "# Retrieve Medical Documents\n",
    "def retrieve_medical_documents(query_tokens, ix, documents):\n",
    "    \"\"\"\n",
    "    Simple keyword matching for documents.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Simple keyword search\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_lower = doc.lower()\n",
    "        for token in query_tokens:\n",
    "            if token in doc_lower:\n",
    "                results.append((f\"doc{i}\", doc))\n",
    "                break\n",
    "\n",
    "    return results[:10]  # Return top 10 results\n",
    "\n",
    "# Summarize Documents\n",
    "def summarize_document(doc_content):\n",
    "    sentences = doc_content.split(\". \")\n",
    "    return sentences[0] + \".\"\n",
    "\n",
    "# Rank Documents by Medical Relevance\n",
    "def rank_medical_documents(query_tokens, retrieved_docs):\n",
    "    docs_content = [doc[1] for doc in retrieved_docs]\n",
    "    if not docs_content:\n",
    "        return retrieved_docs\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs_content + [\" \".join(query_tokens)])\n",
    "    scores = tfidf_matrix[-1].dot(tfidf_matrix[:-1].T).toarray()[0]\n",
    "    ranked_docs = [(retrieved_docs[i][0], retrieved_docs[i][1], scores[i]) for i in range(len(retrieved_docs))]\n",
    "    ranked_docs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return [(title, content) for title, content, score in ranked_docs]\n",
    "\n",
    "# Main CLIR Function\n",
    "def punjabi_english_clir(query, documents):\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    # Step 1: Translate Punjabi query to English\n",
    "    translated_query = translate_text(query, \"pan_Guru\", \"eng_Latn\", indic_en_model, indic_en_tokenizer, ip)\n",
    "    print(f\"\\n[Step 1] Translated Query (Punjabi → English): {translated_query}\")\n",
    "\n",
    "    if not translated_query:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (Translation failed)\"]\n",
    "\n",
    "\n",
    "    # Step 2: Extract relevant query tokens - simplify to focus on medical terms\n",
    "    if \"diabetes\" in translated_query.lower():\n",
    "        query_tokens = [\"diabetes\"]\n",
    "    elif \"hypertension\" in translated_query.lower() or \"blood pressure\" in translated_query.lower():\n",
    "        query_tokens = [\"hypertension\", \"blood pressure\"]\n",
    "    else:\n",
    "        query_tokens = preprocess_text(translated_query, language=\"en\")\n",
    "    print(f\"\\n[Step 2] Extracted Query Tokens: {query_tokens}\")\n",
    "\n",
    "    # Step 3: Use simple expansion of medical terms\n",
    "    medical_terms = expand_medical_terms(query_tokens)\n",
    "    print(f\"\\n[Step 3] Expanded Medical Terms: {medical_terms}\")\n",
    "\n",
    "    # Step 4: Skip complex indexing and directly search documents\n",
    "    retrieved_docs = retrieve_medical_documents(medical_terms, None, documents)\n",
    "    print(\"\\n[Step 5] Retrieved Documents (before ranking):\")\n",
    "    for i, (title, content) in enumerate(retrieved_docs, 1):\n",
    "        print(f\"{i}. {title}: {content}\")\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ।\"]\n",
    "\n",
    "    # Step 6: Rank documents based on medical relevance\n",
    "    ranked_docs = rank_medical_documents(medical_terms, retrieved_docs)\n",
    "    print(\"\\n[Step 6] Ranked Documents:\")\n",
    "    for i, (title, content) in enumerate(ranked_docs[:5], 1):\n",
    "        print(f\"{i}. {title}: {content}\")\n",
    "\n",
    "    # Step 7: Summarize and translate retrieved documents back to Punjabi\n",
    "    punjabi_summaries = []\n",
    "    print(\"\\n[Step 7] Translated Summaries (English → Punjabi):\")\n",
    "    for i, (title, content) in enumerate(ranked_docs[:5], 1):\n",
    "        summary_en = summarize_document(content)\n",
    "        summary_pa = translate_text(summary_en, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "        if summary_pa:\n",
    "            print(f\"{i}. {summary_pa}\")\n",
    "            punjabi_summaries.append(f\"{i}. {summary_pa}\")\n",
    "        else:\n",
    "            print(f\"{i}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "            punjabi_summaries.append(f\"{i}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "\n",
    "    return punjabi_summaries\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Sample Punjabi medical query\n",
    "    punjabi_query = \"ਮੈਂ ਹਾਲ ਹੀ ਵਿੱਚ ਮਧੁਮੇਹ ਦੀ ਪਛਾਣ ਹੋਈ ਹੈ, ਅਤੇ ਮੈਨੂੰ ਸਮਝ ਨਹੀਂ ਆਉਂਦਾ ਕਿ ਆਪਣੀ ਡਾਇਟ ਕਿਵੇਂ ਪ੍ਰਬੰਧਿਤ ਕਰਨੀ ਹੈ। ਕੀ ਤੁਸੀਂ ਮੈਨੂੰ ਦੱਸ ਸਕਦੇ ਹੋ ਕਿ ਮੈਨੂੰ ਕਿਸ ਤਰ੍ਹਾਂ ਦੇ ਖਾਣ-ਪੀਣ ਦੀ ਆਦਤ ਬਣਾਉਣੀ ਚਾਹੀਦੀ ਹੈ ਅਤੇ ਕਿਹੜੀਆਂ ਚੀਜ਼ਾਂ ਤੋਂ ਪਰਹੇਜ਼ ਕਰਨਾ ਚਾਹੀਦਾ ਹੈ?\"\n",
    "\n",
    "    # Sample medical documents (replace with real documents)\n",
    "    documents = [\n",
    "    \"High blood pressure, also called hypertension, occurs when the force of blood against artery walls is too high. It is often managed with lifestyle changes and medications.\",\n",
    "    \"Hypertension is a common condition that can lead to severe health complications such as heart disease and stroke if left untreated.\",\n",
    "    \"Beta-blockers, ACE inhibitors, and calcium channel blockers are some of the most commonly prescribed medications for controlling high blood pressure.\",\n",
    "    \"A low-sodium diet, regular exercise, and weight management are crucial for reducing hypertension risk.\",\n",
    "    \"Symptoms of high blood pressure are often unnoticed, but severe hypertension can cause headaches, shortness of breath, and nosebleeds.\",\n",
    "    \"Doctors recommend monitoring blood pressure regularly to avoid complications like kidney failure and cardiovascular diseases.\",\n",
    "    \"Hypertension treatment includes lifestyle modifications along with medications such as diuretics and angiotensin receptor blockers (ARBs).\",\n",
    "    \"Excessive salt intake, stress, and lack of physical activity are major contributing factors to high blood pressure.\",\n",
    "    \"Managing high blood pressure involves a combination of dietary changes, physical exercise, and prescribed medication.\",\n",
    "    \"Hypertension is known as the 'silent killer' because it often has no symptoms but can damage organs over time.\",\n",
    "    \"Diabetes is a chronic disease that affects how the body processes blood sugar (glucose). Type 1 and Type 2 diabetes are the most common forms.\",\n",
    "    \"Insulin therapy is essential for Type 1 diabetes patients, while Type 2 diabetes can often be managed with oral medications and lifestyle changes.\",\n",
    "    \"Common symptoms of diabetes include frequent urination, excessive thirst, unexplained weight loss, and blurred vision.\",\n",
    "    \"Metformin is one of the first-line medications prescribed for Type 2 diabetes to help regulate blood sugar levels.\",\n",
    "    \"Uncontrolled diabetes can lead to complications such as nerve damage, kidney disease, and cardiovascular problems.\",\n",
    "    \"A balanced diet, regular exercise, and medication adherence are critical for diabetes management.\",\n",
    "    \"Blood sugar monitoring helps patients manage their diabetes effectively and avoid hyperglycemia or hypoglycemia.\",\n",
    "    \"Diabetes increases the risk of other conditions, including hypertension and heart disease.\",\n",
    "    \"Gestational diabetes occurs during pregnancy and increases the risk of developing Type 2 diabetes later in life.\",\n",
    "    \"Insulin resistance is a key factor in Type 2 diabetes and is often linked to obesity and lack of physical activity.\"\n",
    "]\n",
    "\n",
    "    # Run the Punjabi-English CLIR system\n",
    "    results = punjabi_english_clir(punjabi_query, documents)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\nPunjabi Query Results:\")\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "# Ensure the script runs only when executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "8ff95ce2e1f84098",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/charupatelbaghi/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
      "/Users/charupatelbaghi/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translated Query (English): What medications can I take for diabetes?\n",
      "\n",
      "Preprocessed Tokens: ['medications', 'take', 'diabetes']\n",
      "\n",
      "Expanded Tokens: ['blood sugar', 'drug', 'treatment', 'medication', 'diabetes', 'take', 'medications', 'prescription', 'insulin', 'diabetic']\n",
      "\n",
      "Retrieved Documents with Scores:\n",
      "1. Score: 0.2813 | Content: The best medications for diabetes treatment include Metformin and Insulin.\n",
      "2. Score: 0.2332 | Content: Metformin is a widely used prescription drug for diabetes management.\n",
      "3. Score: 0.1970 | Content: People with diabetes should monitor their blood sugar regularly.\n",
      "4. Score: 0.1860 | Content: Patients with high blood sugar levels should consult a doctor for appropriate medications.\n",
      "5. Score: 0.1815 | Content: Insulin therapy is essential for some diabetic patients.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:25:18.612370Z",
     "start_time": "2025-03-16T14:25:18.112372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT\n",
    "from whoosh.qparser import QueryParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import os\n",
    "import nltk\n",
    "from spacy import displacy\n",
    "from openai import OpenAI\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load SciSpacy Medical Model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to query OpenAI API for medical information\n",
    "def query_openai_for_medical_info(query):\n",
    "    \"\"\"\n",
    "    Query OpenAI API for medical information based on the query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Provide medical information about: {query}. Include only factual information and mention any important medical resources or websites where patients can find more information.\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying OpenAI API: {e}\")\n",
    "        return \"Error retrieving information from AI.\"\n",
    "\n",
    "# Extract any URLs mentioned in the AI response\n",
    "def extract_urls(text):\n",
    "    \"\"\"\n",
    "    Extract URLs from the text response.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    urls = url_pattern.findall(text)\n",
    "    return urls\n",
    "\n",
    "# Load Translation Models\n",
    "def load_translation_models():\n",
    "    en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n",
    "\n",
    "    en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, None)\n",
    "    indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, None)\n",
    "\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    return (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip\n",
    "\n",
    "# Initialize model and tokenizer (placeholder function)\n",
    "def initialize_model_and_tokenizer(model_path, device):\n",
    "    # This is a placeholder function - you would need to implement the actual initialization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    if device:\n",
    "        model = model.to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "# IndicProcessor class (placeholder)\n",
    "class IndicProcessor:\n",
    "    def __init__(self, inference=False):\n",
    "        # This is a placeholder\n",
    "        self.inference = inference\n",
    "\n",
    "# Translate text (placeholder function)\n",
    "def translate_text(text, src_lang, tgt_lang, model, tokenizer, processor):\n",
    "    # This is a placeholder function - you would need to implement the actual translation\n",
    "    # For now, let's just return a mock translation\n",
    "    if src_lang == \"pan_Guru\" and tgt_lang == \"eng_Latn\":\n",
    "        # Punjabi to English\n",
    "        return \"I was recently diagnosed with diabetes, and I don't understand how to manage my diet. Can you tell me what kind of eating habits I should develop and what things I should avoid?\"\n",
    "    elif src_lang == \"eng_Latn\" and tgt_lang == \"pan_Guru\":\n",
    "        # English to Punjabi (mock translation)\n",
    "        return \"ਅੰਗਰੇਜ਼ੀ ਤੋਂ ਪੰਜਾਬੀ ਵਿੱਚ ਅਨੁਵਾਦ: \" + text[:50] + \"...\"\n",
    "    return text\n",
    "\n",
    "# Extract Medical Terms using SciSpacy\n",
    "def extract_medical_terms(text):\n",
    "    \"\"\"Extract medical terms with additional specific terms.\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Get terms identified by SciSpacy\n",
    "    medical_terms = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"DISEASE\", \"DRUG\", \"TREATMENT\"]]\n",
    "\n",
    "    # Add specific medical terms we know should be included\n",
    "    known_terms = [\"diabetes\", \"hypertension\", \"blood pressure\", \"sugar\", \"insulin\"]\n",
    "    for term in known_terms:\n",
    "        if term in text.lower() and term not in medical_terms:\n",
    "            medical_terms.append(term)\n",
    "\n",
    "    return list(set(medical_terms))  # Remove duplicates\n",
    "\n",
    "# Expand Medical Terms using WordNet\n",
    "def expand_medical_terms(medical_terms):\n",
    "    \"\"\"\n",
    "    Simple, focused expansion of medical terms.\n",
    "    \"\"\"\n",
    "    expanded_terms = set()\n",
    "    for term in medical_terms:\n",
    "        # Only add the original term and simple variations\n",
    "        expanded_terms.add(term)\n",
    "        # For medical conditions, add singular/plural variations\n",
    "        if term == \"diabetes\":\n",
    "            expanded_terms.add(\"diabetic\")\n",
    "            expanded_terms.add(\"sugar\")  # Common term for diabetes\n",
    "        elif term == \"hypertension\":\n",
    "            expanded_terms.add(\"high blood pressure\")\n",
    "            expanded_terms.add(\"blood pressure\")\n",
    "\n",
    "    return list(expanded_terms)\n",
    "\n",
    "# Preprocess Text with Medical Focus\n",
    "def preprocess_text(text, language=\"en\"):\n",
    "    \"\"\"\n",
    "    Extract only the most relevant tokens from text.\n",
    "    \"\"\"\n",
    "    # Focus on key medical terms\n",
    "    if \"diabetes\" in text.lower():\n",
    "        return [\"diabetes\", \"diabetic\", \"sugar\"]\n",
    "    elif \"hypertension\" in text.lower() or \"blood pressure\" in text.lower():\n",
    "        return [\"hypertension\", \"high blood pressure\", \"blood pressure\"]\n",
    "\n",
    "    # If no specific conditions found, extract medical entities\n",
    "    doc = nlp(text)\n",
    "    medical_terms = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"DISEASE\", \"DRUG\", \"TREATMENT\"]]\n",
    "\n",
    "    # If still no medical terms, fall back to regular tokenization\n",
    "    if not medical_terms:\n",
    "        if language == \"pa\":\n",
    "            tokens = indic_tokenize.trivial_tokenize(text, lang=\"pa\")\n",
    "            stop_words = set([\"ਅਤੇ\", \"ਹੈ\", \"ਵਿੱਚ\", \"ਨੂੰ\"])\n",
    "            tokens = [t.lower() for t in tokens if t.lower() not in stop_words and t.isalnum()]\n",
    "        else:\n",
    "            tokens = word_tokenize(text)\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            # Only keep substantive words (ignore common verbs, etc.)\n",
    "            tokens = [t.lower() for t in tokens if t.lower() not in stop_words and len(t) > 3 and t.isalnum()]\n",
    "        return tokens\n",
    "\n",
    "    return medical_terms\n",
    "\n",
    "# Indexing Documents\n",
    "def create_index(documents, index_dir=\"index\"):\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "    schema = Schema(title=TEXT(stored=True), content=TEXT(stored=True))\n",
    "    ix = create_in(index_dir, schema)\n",
    "    writer = ix.writer()\n",
    "    for i, doc in enumerate(documents):\n",
    "        writer.add_document(title=f\"doc{i}\", content=doc)\n",
    "    writer.commit()\n",
    "    return ix\n",
    "\n",
    "# Retrieve Medical Documents\n",
    "def retrieve_medical_documents(query_tokens, ix, documents):\n",
    "    \"\"\"\n",
    "    Simple keyword matching for documents.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Simple keyword search\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_lower = doc.lower()\n",
    "        for token in query_tokens:\n",
    "            if token in doc_lower:\n",
    "                results.append((f\"doc{i}\", doc))\n",
    "                break\n",
    "\n",
    "    return results[:10]  # Return top 10 results\n",
    "\n",
    "# Summarize Documents\n",
    "def summarize_document(doc_content):\n",
    "    sentences = doc_content.split(\". \")\n",
    "    return sentences[0] + \".\"\n",
    "\n",
    "# Rank Documents by Medical Relevance\n",
    "def rank_medical_documents(query_tokens, retrieved_docs):\n",
    "    docs_content = [doc[1] for doc in retrieved_docs]\n",
    "    if not docs_content:\n",
    "        return retrieved_docs\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs_content + [\" \".join(query_tokens)])\n",
    "    scores = tfidf_matrix[-1].dot(tfidf_matrix[:-1].T).toarray()[0]\n",
    "    ranked_docs = [(retrieved_docs[i][0], retrieved_docs[i][1], scores[i]) for i in range(len(retrieved_docs))]\n",
    "    ranked_docs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return [(title, content) for title, content, score in ranked_docs]\n",
    "\n",
    "# Main CLIR Function with OpenAI integration\n",
    "def punjabi_english_clir_with_openai(query, documents):\n",
    "    (en_indic_tokenizer, en_indic_model), (indic_en_tokenizer, indic_en_model), ip = load_translation_models()\n",
    "\n",
    "    # Step 1: Translate Punjabi query to English\n",
    "    translated_query = translate_text(query, \"pan_Guru\", \"eng_Latn\", indic_en_tokenizer, indic_en_model, ip)\n",
    "    print(f\"\\n[Step 1] Translated Query (Punjabi → English): {translated_query}\")\n",
    "\n",
    "    if not translated_query:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ। (Translation failed)\"]\n",
    "\n",
    "    # Step 2: Query OpenAI for medical information\n",
    "    openai_response = query_openai_for_medical_info(translated_query)\n",
    "    print(f\"\\n[Step 2] OpenAI Response: {openai_response}\")\n",
    "\n",
    "    # Extract any URLs mentioned in the response\n",
    "    urls = extract_urls(openai_response)\n",
    "    print(f\"\\n[Step 3] URLs mentioned: {urls}\")\n",
    "\n",
    "    # Step 4: Extract relevant query tokens\n",
    "    if \"diabetes\" in translated_query.lower():\n",
    "        query_tokens = [\"diabetes\"]\n",
    "    elif \"hypertension\" in translated_query.lower() or \"blood pressure\" in translated_query.lower():\n",
    "        query_tokens = [\"hypertension\", \"blood pressure\"]\n",
    "    else:\n",
    "        query_tokens = preprocess_text(translated_query, language=\"en\")\n",
    "    print(f\"\\n[Step 4] Extracted Query Tokens: {query_tokens}\")\n",
    "\n",
    "    # Step 5: Use simple expansion of medical terms\n",
    "    medical_terms = expand_medical_terms(query_tokens)\n",
    "    print(f\"\\n[Step 5] Expanded Medical Terms: {medical_terms}\")\n",
    "\n",
    "    # Step 6: Retrieve documents and include OpenAI response as a document\n",
    "    documents_with_openai = documents + [openai_response]\n",
    "    retrieved_docs = retrieve_medical_documents(medical_terms, None, documents_with_openai)\n",
    "    print(\"\\n[Step 6] Retrieved Documents (before ranking):\")\n",
    "    for i, (title, content) in enumerate(retrieved_docs, 1):\n",
    "        print(f\"{i}. {title}: {content[:100]}...\")\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return [\"ਕੋਈ ਸੰਬੰਧਿਤ ਜਾਣਕਾਰੀ ਨਹੀਂ ਮਿਲੀ।\"]\n",
    "\n",
    "    # Step 7: Rank documents based on medical relevance\n",
    "    ranked_docs = rank_medical_documents(medical_terms, retrieved_docs)\n",
    "    print(\"\\n[Step 7] Ranked Documents:\")\n",
    "    for i, (title, content) in enumerate(ranked_docs[:5], 1):\n",
    "        print(f\"{i}. {title}: {content[:100]}...\")\n",
    "\n",
    "    # Step 8: Summarize and translate retrieved documents back to Punjabi\n",
    "    punjabi_summaries = []\n",
    "    print(\"\\n[Step 8] Translated Summaries (English → Punjabi):\")\n",
    "\n",
    "    # First, translate the OpenAI response\n",
    "    openai_summary = openai_response[:500]  # Limit to first 500 chars to avoid token limits\n",
    "    openai_punjabi = translate_text(openai_summary, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "\n",
    "    # Add URLs to the translation if any were found\n",
    "    if urls:\n",
    "        url_text = \"\\n\\nਹੋਰ ਜਾਣਕਾਰੀ ਲਈ ਵੈੱਬਸਾਈਟਾਂ: \" + \", \".join(urls)\n",
    "        openai_punjabi += url_text\n",
    "\n",
    "    print(f\"OpenAI Response (Punjabi): {openai_punjabi}\")\n",
    "    punjabi_summaries.append(f\"ਏਆਈ ਜਵਾਬ: {openai_punjabi}\")\n",
    "\n",
    "    # Then translate the other documents\n",
    "    for i, (title, content) in enumerate(ranked_docs[:3], 1):  # Limit to top 3 docs\n",
    "        summary_en = summarize_document(content)\n",
    "        summary_pa = translate_text(summary_en, \"eng_Latn\", \"pan_Guru\", en_indic_model, en_indic_tokenizer, ip)\n",
    "        if summary_pa:\n",
    "            print(f\"{i}. {summary_pa}\")\n",
    "            punjabi_summaries.append(f\"{i}. {summary_pa}\")\n",
    "        else:\n",
    "            print(f\"{i}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "            punjabi_summaries.append(f\"{i}. ਸੰਖੇਪ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀ।\")\n",
    "\n",
    "    return punjabi_summaries\n",
    "\n",
    "def main():\n",
    "    # Sample Punjabi medical query\n",
    "    punjabi_query = \"ਮੈਂ ਹਾਲ ਹੀ ਵਿੱਚ ਮਧੁਮੇਹ ਦੀ ਪਛਾਣ ਹੋਈ ਹੈ, ਅਤੇ ਮੈਨੂੰ ਸਮਝ ਨਹੀਂ ਆਉਂਦਾ ਕਿ ਆਪਣੀ ਡਾਇਟ ਕਿਵੇਂ ਪ੍ਰਬੰਧਿਤ ਕਰਨੀ ਹੈ। ਕੀ ਤੁਸੀਂ ਮੈਨੂੰ ਦੱਸ ਸਕਦੇ ਹੋ ਕਿ ਮੈਨੂੰ ਕਿਸ ਤਰ੍ਹਾਂ ਦੇ ਖਾਣ-ਪੀਣ ਦੀ\"\n",
    "    results = punjabi_english_clir(punjabi_query, documents)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\nPunjabi Query Results:\")\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "# Ensure the script runs only when executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "9afc8b8771a08394",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/charupatelbaghi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOpenAIError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     24\u001B[39m nlp = spacy.load(\u001B[33m\"\u001B[39m\u001B[33men_core_sci_sm\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# Initialize OpenAI Client\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m client = \u001B[43mOpenAI\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[38;5;66;03m# Function to query OpenAI API for medical information\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery_openai_for_medical_info\u001B[39m(query):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/openai/_client.py:114\u001B[39m, in \u001B[36mOpenAI.__init__\u001B[39m\u001B[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001B[39m\n\u001B[32m    112\u001B[39m     api_key = os.environ.get(\u001B[33m\"\u001B[39m\u001B[33mOPENAI_API_KEY\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    113\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m api_key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m OpenAIError(\n\u001B[32m    115\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    116\u001B[39m     )\n\u001B[32m    117\u001B[39m \u001B[38;5;28mself\u001B[39m.api_key = api_key\n\u001B[32m    119\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m organization \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mOpenAIError\u001B[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I am trying to extract the relevant information using We will use SpaCy (NER) + YAKE (Keyword Extraction) + WordNet (Synonyms Expansion).",
   "id": "bbb42944b253a53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T05:56:10.556258Z",
     "start_time": "2025-03-15T05:56:09.951841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import yake\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Load English NLP Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample Query\n",
    "query = \"I have recently been diagnosed with diabetes, and I don't understand how to manage my diet. Can you tell me what kind of eating habits I should make and what things to avoid?\"\n",
    "\n",
    "### STEP 1: Named Entity Recognition (NER)\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"DISEASE\", \"SYMPTOM\", \"FOOD\", \"NUTRIENT\"]]\n",
    "    return list(set(entities))  # Remove duplicates\n",
    "\n",
    "### STEP 2: Keyword Extraction using YAKE\n",
    "def extract_keywords(text):\n",
    "    kw_extractor = yake.KeywordExtractor(lan=\"en\", n=2, top=7)  # Extract 2-word phrases\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "### STEP 3: Synonym Expansion (WordNet)\n",
    "def expand_synonyms(words):\n",
    "    synonyms = set(words)\n",
    "    for word in words:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().replace(\"_\", \" \"))  # Convert underscores to spaces\n",
    "    return list(synonyms)\n",
    "\n",
    "### FINAL FUNCTION: Extract Meaningful Tokens\n",
    "def extract_relevant_tokens(text):\n",
    "    entities = extract_entities(text)\n",
    "    keywords = extract_keywords(text)\n",
    "    expanded_terms = expand_synonyms(entities + keywords)\n",
    "\n",
    "    return list(set(expanded_terms))  # Remove duplicates\n",
    "\n",
    "# Run Token Extraction\n",
    "tokens = extract_relevant_tokens(query)\n",
    "\n",
    "# Print Results\n",
    "print(\"Extracted Tokens:\", tokens)\n"
   ],
   "id": "97efa2a62b33c64d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Tokens: ['wangle', 'diagnosed', 'do', 'diet', 'latterly', 'empathise', 'grapple', 'finagle', 'see', 'name', 'deal', 'handle', 'sympathize', 'contend', 'infer', 'supervise', 'sympathise', 'diagnose', 'manage', 'of late', 'get by', 'care', 'read', 'oversee', 'wield', 'understand', 'lately', 'make do', 'diabetes', 'superintend', 'interpret', 'carry off', 'realise', 'pull off', 'bring off', 'realize', 'empathize', 'recently', 'translate', 'make out', 'cope', 'late', 'eating habits', 'dieting', 'negociate']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8b3b0e3b11815e91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "what I'm trying, is I'm training a model based on the data set medquad_answers.csv file and then get the relevant results",
   "id": "e7c2c4abdb148ce0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T07:37:24.745439Z",
     "start_time": "2025-03-16T07:37:22.227185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 1: Parse and prepare the dataset by extracting questions and answers\n",
    "def parse_data(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Initialize lists to store extracted questions and answers\n",
    "    question_list = []\n",
    "    answer_list = []\n",
    "    doc_id_list = []\n",
    "\n",
    "    # Iterate through each row and extract the question and answer\n",
    "    for idx, row in df.iterrows():\n",
    "        answer_text = row['Answer']\n",
    "        doc_id = row['AnswerID']\n",
    "\n",
    "        # Extract question using regex pattern - looking for \"Question: \" followed by text\n",
    "        question_match = re.search(r'Question:\\s*(.*?)\\s*(?:URL:|$)', answer_text)\n",
    "        if question_match:\n",
    "            question = question_match.group(1).strip()\n",
    "        else:\n",
    "            question = \"\"\n",
    "\n",
    "        # Extract answer using regex pattern - looking for \"Answer: \" followed by text\n",
    "        answer_match = re.search(r'Answer:\\s*(.*?)(?:$)', answer_text)\n",
    "        if answer_match:\n",
    "            answer = answer_match.group(1).strip()\n",
    "        else:\n",
    "            answer = \"\"\n",
    "\n",
    "        # Add to lists if both question and answer were found\n",
    "        if question and answer:\n",
    "            question_list.append(question)\n",
    "            answer_list.append(answer)\n",
    "            doc_id_list.append(doc_id)\n",
    "\n",
    "    # Create a new DataFrame with separate question and answer columns\n",
    "    processed_df = pd.DataFrame({\n",
    "        'DocID': doc_id_list,\n",
    "        'Question': question_list,\n",
    "        'Answer': answer_list\n",
    "    })\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "# Step 2: Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    # Remove special characters (keep some punctuation for medical terms)\n",
    "    text = re.sub(r'[^\\w\\s.,;?!-]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Download NLTK resources if needed\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "    try:\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "    except LookupError:\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    # Tokenization and stopword removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Keep medical terms intact - don't remove all stopwords for medical text\n",
    "    medical_stopwords = {'and', 'or', 'the', 'a', 'an'}\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens\n",
    "                      if word not in medical_stopwords]\n",
    "\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Step 3: Prepare dataset for training\n",
    "def prepare_dataset(df):\n",
    "    df['processed_question'] = df['Question'].apply(preprocess_text)\n",
    "    df['processed_answer'] = df['Answer'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "# Option 1: Traditional ML approach using TF-IDF and classifier\n",
    "def train_classifier_model(df):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['processed_question'], df['processed_answer'],\n",
    "        test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert text to features using TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train a simple classifier as proof of concept\n",
    "    # In a real scenario, you might need more sophisticated approach for text generation\n",
    "    print(\"Training classifier model...\")\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    predictions = classifier.predict(X_test_tfidf)\n",
    "\n",
    "    # This approach isn't ideal for text generation but can classify similar answers\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Model accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'model': classifier,\n",
    "        'vectorizer': tfidf_vectorizer,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# Option 2: BERT-based approach for Q&A\n",
    "class MedicalQADataset(Dataset):\n",
    "    def __init__(self, questions, answers, tokenizer, max_length=512):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "\n",
    "        # Tokenize inputs\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Get the answer start and end positions\n",
    "        # This is simplified and would need refinement for production\n",
    "        answer_tokens = self.tokenizer(answer, return_tensors='pt')\n",
    "\n",
    "        # For simplicity, we'll just use dummy values for start/end positions\n",
    "        # In a real scenario, you'd need to find the actual positions\n",
    "        start_positions = torch.tensor([1])\n",
    "        end_positions = torch.tensor([len(answer_tokens['input_ids'][0])])\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'start_positions': start_positions.flatten(),\n",
    "            'end_positions': end_positions.flatten()\n",
    "        }\n",
    "\n",
    "def train_bert_model(df):\n",
    "    print(\"Setting up BERT model...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Split data\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = MedicalQADataset(\n",
    "        train_df['processed_question'].tolist(),\n",
    "        train_df['processed_answer'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    # This is a simplified implementation\n",
    "    # In a real scenario, you would:\n",
    "    # 1. Create proper DataLoaders\n",
    "    # 2. Set up training loop with optimizer\n",
    "    # 3. Train for multiple epochs\n",
    "    # 4. Save model checkpoints\n",
    "\n",
    "    print(\"BERT model setup complete. Note: Actual training would require:\")\n",
    "    print(\"1. GPU resources\")\n",
    "    print(\"2. Training loop implementation\")\n",
    "    print(\"3. Proper answer span detection\")\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer\n",
    "    }\n",
    "\n",
    "# Function to extract questions and answers from raw text\n",
    "def extract_qa_from_text(text):\n",
    "    # Extract question\n",
    "    question_match = re.search(r'Question:\\s*(.*?)\\s*(?:URL:|$)', text, re.DOTALL)\n",
    "    question = question_match.group(1).strip() if question_match else \"\"\n",
    "\n",
    "    # Extract answer\n",
    "    answer_match = re.search(r'Answer:\\s*(.*?)(?:$)', text, re.DOTALL)\n",
    "    answer = answer_match.group(1).strip() if answer_match else \"\"\n",
    "\n",
    "    return question, answer\n",
    "\n",
    "# Main function to run the pipeline\n",
    "def main():\n",
    "    # Step 1: Load and parse the data\n",
    "    # Replace with your actual file path\n",
    "    file_path = 'medical_qa_dataset.csv'\n",
    "\n",
    "    print(f\"Step 1: Parsing data from {file_path}...\")\n",
    "\n",
    "    # If you need to create the CSV first from the raw data\n",
    "    # Create a simple CSV with AnswerID and Answer columns\n",
    "    sample_data = {\n",
    "        'AnswerID': ['ADAM_0003147_Sec1.txt', 'ADAM_0003147_Sec2.txt'],\n",
    "        'Answer': [\n",
    "            'Question: What is (are) Polycystic ovary syndrome? (Also called: Polycystic ovaries; Polycystic ovary disease; Stein-Leventhal syndrome; Polyfollicular ovarian disease) URL: https://www.nlm.nih.gov/medlineplus/ency/article/000369.htm Answer: Polycystic ovary syndrome is a condition in which a woman has an imbalance of female sex hormones.',\n",
    "            'Question: What causes Polycystic ovary syndrome? (Also called: Polycystic ovaries; Polycystic ovary disease; Stein-Leventhal syndrome; Polyfollicular ovarian disease) URL: https://www.nlm.nih.gov/medlineplus/ency/article/000369.htm Answer: PCOS is linked to changes in hormone levels that make it harder for the ovaries to release fully-grown eggs.'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # For demo purposes, we'll create a dataframe from sample data\n",
    "    # In a real scenario, replace this with reading from your CSV file:\n",
    "    # df_raw = pd.read_csv(file_path)\n",
    "    df_raw = pd.DataFrame(sample_data)\n",
    "\n",
    "    # Step 2: Extract questions and answers from the data\n",
    "    print(\"Step 2: Extracting questions and answers...\")\n",
    "    processed_df = pd.DataFrame(columns=['DocID', 'Question', 'Answer'])\n",
    "\n",
    "    for idx, row in df_raw.iterrows():\n",
    "        doc_id = row['AnswerID']\n",
    "        question, answer = extract_qa_from_text(row['Answer'])\n",
    "        if question and answer:\n",
    "            new_row = pd.DataFrame({\n",
    "                'DocID': [doc_id],\n",
    "                'Question': [question],\n",
    "                'Answer': [answer]\n",
    "            })\n",
    "            processed_df = pd.concat([processed_df, new_row], ignore_index=True)\n",
    "\n",
    "    print(f\"Extracted {len(processed_df)} question-answer pairs\")\n",
    "    print(\"Sample Q&A pair:\")\n",
    "    print(f\"Q: {processed_df['Question'].iloc[0]}\")\n",
    "    print(f\"A: {processed_df['Answer'].iloc[0]}\")\n",
    "\n",
    "    # Step 3: Preprocess the data\n",
    "    print(\"Step 3: Preprocessing text data...\")\n",
    "    processed_df = prepare_dataset(processed_df)\n",
    "\n",
    "    # Step 4: Choose and train model\n",
    "    print(\"Step 4: Training model...\")\n",
    "    # Option 1: Traditional ML approach (faster)\n",
    "    classifier_results = train_classifier_model(processed_df)\n",
    "\n",
    "    # Option 2: BERT approach (better but requires more resources)\n",
    "    # Uncomment to use BERT (requires GPU and more time)\n",
    "    # bert_results = train_bert_model(processed_df)\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # Step 5: Save models (in a real implementation)\n",
    "    print(\"Step 5: In a real implementation, models would be saved here\")\n",
    "\n",
    "    # Step 6: Create simple inference function\n",
    "    print(\"Example of using the trained model:\")\n",
    "\n",
    "    def predict_answer(question, model=classifier_results['model'],\n",
    "                       vectorizer=classifier_results['vectorizer']):\n",
    "        # Preprocess question\n",
    "        processed_q = preprocess_text(question)\n",
    "        # Vectorize\n",
    "        q_vector = vectorizer.transform([processed_q])\n",
    "        # Predict\n",
    "        prediction = model.predict(q_vector)[0]\n",
    "        return prediction\n",
    "\n",
    "    # Test with a question\n",
    "    test_question = \"What is Polycystic ovary syndrome?\"\n",
    "    predicted_answer = predict_answer(test_question)\n",
    "    print(f\"Q: {test_question}\")\n",
    "    print(f\"Predicted A: {predicted_answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "b4c9f1cb242738f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Parsing data from medical_qa_dataset.csv...\n",
      "Step 2: Extracting questions and answers...\n",
      "Extracted 2 question-answer pairs\n",
      "Sample Q&A pair:\n",
      "Q: What is (are) Polycystic ovary syndrome? (Also called: Polycystic ovaries; Polycystic ovary disease; Stein-Leventhal syndrome; Polyfollicular ovarian disease)\n",
      "A: Polycystic ovary syndrome is a condition in which a woman has an imbalance of female sex hormones.\n",
      "Step 3: Preprocessing text data...\n",
      "Step 4: Training model...\n",
      "Training classifier model...\n",
      "Model accuracy: 0.0000\n",
      "Training complete!\n",
      "Step 5: In a real implementation, models would be saved here\n",
      "Example of using the trained model:\n",
      "Q: What is Polycystic ovary syndrome?\n",
      "Predicted A: polycystic ovary syndrome is condition in which woman ha imbalance of female sex hormone .\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:02:35.956345Z",
     "start_time": "2025-03-17T07:02:32.846255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s.,;?!-]', '', text).lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens if word not in stop_words])\n",
    "\n",
    "def train_and_evaluate_model(df, model_type='knn'):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['processed_question'], df['processed_answer'], test_size=0.2, random_state=42\n",
    "    )\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=5) if model_type == 'knn' else RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "    exact_match_accuracy = accuracy_score(y_test, predictions)\n",
    "    bleu_scores = [sentence_bleu([nltk.word_tokenize(ref.lower())], nltk.word_tokenize(pred.lower())) for ref, pred in zip(y_test, predictions)]\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "    print(f\"Accuracy: {exact_match_accuracy:.4f}\")\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.4f}\\n\")\n",
    "\n",
    "    return model, tfidf_vectorizer\n",
    "\n",
    "def create_inference_function(model, vectorizer):\n",
    "    def predict_answer(question):\n",
    "        processed_q = preprocess_text(question)\n",
    "        q_vector = vectorizer.transform([processed_q])\n",
    "        return model.predict(q_vector)[0]\n",
    "    return predict_answer\n",
    "\n",
    "def main():\n",
    "    file_path = 'medquad_answers.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['processed_question'] = df['Question'].apply(preprocess_text)\n",
    "    df['processed_answer'] = df['Answer'].apply(preprocess_text)\n",
    "\n",
    "    model, vectorizer = train_and_evaluate_model(df, model_type='knn')\n",
    "    predict_answer = create_inference_function(model, vectorizer)\n",
    "\n",
    "    new_questions = [\n",
    "        \"What is the treatment for Polycystic ovary syndrome?\",\n",
    "        \"Can Noonan syndrome be diagnosed before birth?\",\n",
    "        \"Are there any dietary recommendations for PCOS patients?\",\n",
    "        \"What is the life expectancy for someone with Noonan syndrome?\",\n",
    "        \"How is Neurofibromatosis-Noonan syndrome different from regular Noonan syndrome?\"\n",
    "    ]\n",
    "\n",
    "    for question in new_questions:\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"Predicted A: {predict_answer(question)}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "edeb374774c325da",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Question'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3804\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3805\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3806\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mindex.pyx:167\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mindex.pyx:196\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mKeyError\u001B[39m: 'Question'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 81\u001B[39m\n\u001B[32m     78\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPredicted A: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpredict_answer(question)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     80\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 62\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     60\u001B[39m file_path = \u001B[33m'\u001B[39m\u001B[33mmedquad_answers.csv\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m     61\u001B[39m df = pd.read_csv(file_path)\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mprocessed_question\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mQuestion\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m.apply(preprocess_text)\n\u001B[32m     63\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mprocessed_answer\u001B[39m\u001B[33m'\u001B[39m] = df[\u001B[33m'\u001B[39m\u001B[33mAnswer\u001B[39m\u001B[33m'\u001B[39m].apply(preprocess_text)\n\u001B[32m     65\u001B[39m model, vectorizer = train_and_evaluate_model(df, model_type=\u001B[33m'\u001B[39m\u001B[33mknn\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4100\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.nlevels > \u001B[32m1\u001B[39m:\n\u001B[32m   4101\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._getitem_multilevel(key)\n\u001B[32m-> \u001B[39m\u001B[32m4102\u001B[39m indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4103\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[32m   4104\u001B[39m     indexer = [indexer]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3807\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   3808\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc.Iterable)\n\u001B[32m   3809\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[32m   3810\u001B[39m     ):\n\u001B[32m   3811\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[32m-> \u001B[39m\u001B[32m3812\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m   3814\u001B[39m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[32m   3815\u001B[39m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[32m   3816\u001B[39m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[32m   3817\u001B[39m     \u001B[38;5;28mself\u001B[39m._check_indexing_error(key)\n",
      "\u001B[31mKeyError\u001B[39m: 'Question'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5585a19e69e72b7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
